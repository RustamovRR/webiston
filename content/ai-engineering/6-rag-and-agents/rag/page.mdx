# Qidiruv bilan boyitilgan generatsiya (_RAG_)

Qidiruv bilan boyitilgan generatsiya (_RAG_) — bu tashqi xotira manbalaridan tegishli ma'lumotlarni qidirib topish orqali modelning generatsiya qobiliyatini kuchaytiradigan texnikadir. Tashqi xotira manbasi sifatida ichki ma'lumotlar bazasi, foydalanuvchining avvalgi chat sessiyalari yoki internet xizmat qilishi mumkin.

"Qidirib-yaratish" (_retrieve-then-generate_) andozasi ilk bor [Chen va boshqalar (2017)](https://arxiv.org/abs/1704.00051) tomonidan "Reading Wikipedia to Answer Open-Domain Questions" maqolasida taqdim etilgan. Ushbu ishda tizim avval savolga eng aloqador bo'lgan beshta Wikipedia sahifasini qidirib topadi, so'ngra model[^1] (6-1-rasmda ko'rsatilganidek) javobni shakllantirish uchun ushbu sahifalardagi ma'lumotlardan foydalanadi (yoki ularni "o'qiydi").

![6-1-rasm. "Qidirib-yaratish" andozasi. Model bu yerda hujjat o'quvchi (document reader) deb atalgan.](/ai-engineering/6-chapter/6.1-figure.png)
<div className='text-center text-sm italic'>6-1-rasm. "Qidirib-yaratish" andozasi. Model bu yerda hujjat o'quvchi (document reader) deb atalgan.</div>

_Retrieval-augmented generation_ atamasi "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" ([Lewis va boshqalar, 2020](https://arxiv.org/abs/2005.11401)) maqolasida ilmiy muomalaga kiritilgan. Maqola mualliflari _RAG_'ni barcha mavjud bilimlarni to'g'ridan-to'g'ri modelga sig'dirib bo'lmaydigan, chuqur bilim talab qiluvchi vazifalar uchun yechim sifatida taklif qilishgan. _RAG_ yordamida faqat so'rovga eng aloqador bo'lgan va qidiruvchi (_retriever_) tomonidan aniqlangan ma'lumotlargina ajratib olinadi va modelga uzatiladi. Lewis va uning hamkasblari tegishli ma'lumotlarga ega bo'lish modelga batafsilroq javoblar yaratishga yordam berishini va gallyutsinatsiyalarni kamaytirishini aniqladilar.[^2]

Misol uchun, "Acme'ning `fancy-printer-A300` modeli 100pps tezlikda chop eta oladimi?" degan so'rov berilganda, agar modelga `fancy-printer-A300`'ning texnik xususiyatlari taqdim etilsa, u ancha yaxshiroq javob bera oladi.[^3]

Siz _RAG_'ni barcha so'rovlar uchun bir xil umumiy kontekstni ishlatish o'rniga, har bir so'rov uchun maxsus kontekstni shakllantirish texnikasi deb tushunishingiz mumkin. Bu foydalanuvchi ma'lumotlarini boshqarishda qo'l keladi, chunki u sizga ma'lum bir foydalanuvchiga tegishli ma'lumotlarni faqat shu foydalanuvchi bilan bog'liq so'rovlarga qo'shish imkonini beradi.

Fundamental modellar uchun kontekstni shakllantirish — bu klassik _ML_ modellaridagi **belgilarni shakllantirish** (_feature engineering_) jarayonining o'zginasidir. Ular ayni bir maqsadga xizmat qiladi: modelga kiruvchi ma'lumotni qayta ishlash uchun zarur bo'lgan axborotni taqdim etish.

Fundamental modellarning ilk davrlarida _RAG_ eng keng tarqalgan andozalardan biri sifatida maydonga chiqdi. Uning asosiy vazifasi modellarning kontekst cheklovlarini yengib o'tish edi. Ko'pchilik yetarlicha uzun kontekst _RAG_'ning davrini tugatadi deb hisoblaydi. Men bu fikrga qo'shilmayman. Birinchidan, modelning kontekst uzunligi qanchalik katta bo'lmasin, undan ham uzunroq kontekstni talab qiladigan ilovalar har doim topiladi. Boz ustiga, mavjud ma'lumotlar hajmi vaqt o'tishi bilan faqat o'sib boradi. Odamlar yangi ma'lumotlarni yaratadilar va qo'shadilar, lekin kamdan-kam hollarda o'chiradilar. Kontekst uzunligi tez kengaymoqda, ammo turli xil ilovalarning ma'lumotlarga bo'lgan ehtiyojlarini qondirish uchun yetarli darajada tez emas.[^4]

Ikkinchidan, uzun kontekstni qayta ishlay oladigan model bu kontekstdan har doim ham unumli foydalanavermayd(["Kontekst hajmi va undan foydalanish samaradorligi"](/books/ai-engineering/5-prompt-engineering/introduction-to-prompting#kontekst-hajmi-va-undan-foydalanish-samaradorligi) bo'limida muhokama qilinganidek). Kontekst qanchalik uzun bo'lsa, modelning e'tiborni noto'g'ri qismga qaratish ehtimoli shunchalik ortadi. Har bir qo'shimcha kontekst tokeni ortiqcha xarajat va potensial qo'shimcha kechikishni (_latency_) keltirib chiqaradi. _RAG_ modelga har bir so'rov uchun faqat eng kerakli ma'lumotlardan foydalanish imkonini beradi, bu esa kiruvchi tokenlar sonini kamaytiradi va ayni paytda modelning ishlash samaradorligini oshirishi mumkin.

Kontekst uzunligini kengaytirishga qaratilgan sa'y-harakatlar modellarni kontekstdan samaraliroq foydalanishga o'rgatish ishlari bilan parallel ravishda ketmoqda. Agar biror model provayderi modelga kontekstning eng muhim qismlarini ajratib olishga yordam beradigan qidiruvga o'xshash (_retrieval-like_) yoki e'tiborga asoslangan (_attention-like_) mexanizmni joriy qilsa, men bundan ajablanmagan bo'lardim.

<Callout>
#### Eslatma

Anthropic ta'kidlashicha, `Claude` modellari uchun agar "bilimlar bazangiz 200 000 tokendan (taxminan 500 sahifa material) kichik bo'lsa, siz shunchaki butun bazani modelga beriladigan prompt ichiga kiritishingiz mumkin, bunda _RAG_ yoki shunga o'xshash usullarga hojat qolmaydi" ([Anthropic, 2024](https://www.anthropic.com/engineering/contextual-retrieval)). Boshqa model ishlab chiquvchilari ham o'z modellari uchun _RAG_ va uzun kontekst o'rtasidagi tanlov bo'yicha shunday ko'rsatmalar berishsa, ajoyib ish bo'lar edi.
</Callout>

## _RAG_ arxitekturasi

_RAG_ tizimi ikki komponentdan iborat: tashqi xotira manbalaridan ma'lumotlarni qidirib topuvchi **qidiruvchi** (_retriever_) va topilgan ma'lumotlarga asoslanib javob shakllantiruvchi **generator**. 6-2-rasmda _RAG_ tizimining yuqori darajadagi arxitekturasi tasvirlangan.

![6-2-rasm. Asosiy RAG arxitekturasi.](/ai-engineering/6-chapter/6.2-figure.png)
<div className='text-center text-sm italic'>6-2-rasm. Asosiy RAG arxitekturasi.</div>

Asl _RAG_ maqolasida [Lewis va uning jamoasi](https://arxiv.org/abs/2005.11401) qidiruvchi va generativ modelni birgalikda o'qitishgan. Bugungi kunda esa bu ikki qism ko'pincha alohida o'qitiladi va ko'plab jamoalar o'z _RAG_ tizimlarini tayyor qidiruvchilar va modellardan foydalangan holda quradilar. Biroq, butun _RAG_ tizimini boshdan-oyoq (_end-to-end_) _finetuning_ qilish uning samaradorligini sezilarli darajada oshirishi mumkin.

_RAG_ tizimining muvaffaqiyati uning qidiruvchisi sifatiga bog'liq. Qidiruvchi ikkita asosiy funksiyani bajaradi: **indekslash** (_indexing_) va **so'rov yuborish** (_querying_). Indekslash ma'lumotlarni keyinchalik tez qidirib topish imkonini beradigan tarzda qayta ishlashni o'z ichiga oladi. Unga aloqador ma'lumotlarni olish uchun so'rov yuborish jarayoni esa so'rov yuborish deb ataladi. Ma'lumotlarni qanday indekslash ularni keyinchalik qanday qidirib topmoqchi ekanligingizga bog'liq.

Asosiy komponentlarni ko'rib chiqdik, endi keling, _RAG_ tizimi qanday ishlashini bir misol yordamida tahlil qilaylik. Soddalik uchun, tashqi xotirani kompaniyaning eslatmalari, shartnomalari va majlis bayonnomalari kabi hujjatlar bazasi deb tasavvur qilamiz. Bir hujjat 10 tokendan yoki 1 million tokendan iborat bo'lishi mumkin. Butun boshli hujjatlarni shunchaki qidirib olish kontekstingizning haddan tashqari uzayib ketishiga olib kelishi mumkin. Buning oldini olish uchun har bir hujjatni kichikroq, boshqarish oson bo'lgan **bo'laklarga** (_chunks_) ajratishingiz mumkin. Bo'laklash (_chunking_) strategiyalari ushbu bobda keyinroq muhokama qilinadi. Hozircha barcha hujjatlar ishlashga yaroqli bo'laklarga ajratilgan deb faraz qilamiz. Har bir so'rov uchun bizning maqsadimiz — aynan shu so'rovga eng aloqador ma'lumot bo'laklarini qidirib topishdir. Topilgan ma'lumot bo'laklarini foydalanuvchi prompti bilan birlashtirib, yakuniy promptni hosil qilish uchun ko'pincha kichik yakuniy ishlov berish (_post-processing_) talab etiladi. So'ngra ushbu yakuniy prompt generativ modelga uzatiladi.

<Callout>
#### Eslatma

Ushbu bobda men "hujjat" va "bo'lak" tushunchalarini ifodalash uchun "hujjat" atamasidan foydalanaman, chunki texnik jihatdan hujjatning bir bo'lagi ham o'z-o'zidan hujjat hisoblanadi. Men buni kitobdagi terminologiyani klassik _NLP_ va axborot qidiruvi (_IR_) atamalari bilan muvofiqlashtirish maqsadida qilyapman
</Callout>

## Qidiruv algoritmlari

Qidiruv faqat _RAG_'ga xos narsa emas. Axborot qidiruvi (_Information retrieval_) — bu bir asrlik tarixga ega g'oya.[^5] U qidiruv tizimlari, tavsiya tizimlari, loglar tahlili va boshqalarning umurtqa pog'onasidir. An'anaviy qidiruv tizimlari uchun ishlab chiqilgan ko'plab algoritmlar _RAG_ uchun ham ishlatilishi mumkin. Misol uchun, axborot qidiruvi — bu bir necha sahifada to'liq yoritib berish imkonsiz bo'lgan, ortida ulkan industriya turgan sermahsul tadqiqot yo'nalishidir. Shu sababli, bu bo'limda faqat asosiy chizgilarga to'xtalamiz. Axborot qidiruvi bo'yicha chuqurroq manbalarni ushbu kitobning [GitHub repozitoriysidan](https://github.com/chiphuyen/aie-book) topishingiz mumkin.

<Callout>
#### Eslatma

Qidiruv (_retrieval_) odatda bitta ma'lumotlar bazasi yoki tizim bilan cheklanadi, izlash (_search_) esa turli tizimlar bo'ylab qidiruvni o'z ichiga oladi. Ushbu bobda qidiruv va izlash atamalari bir-birining o'rnida ishlatiladi.
</Callout>

Mohiyatan, qidiruv hujjatlarni berilgan so'rovga aloqadorligiga qarab darajalash (_ranking_) orqali ishlaydi. Qidiruv algoritmlari aloqadorlik ballari qanday hisoblanishiga qarab farqlanadi. Men ikkita keng tarqalgan qidiruv mexanizmidan boshlayman: **atamalarga asoslangan qidiruv** (_term-based retrieval_) va **_embedding_'ga asoslangan qidiruv** (_embedding-based retrieval_).

> #### Siyrak va zich qidiruv
>
> Adabiyotlarda qidiruv algoritmlarining quyidagi toifalarga bo'linganiga duch kelishingiz mumkin: siyrak (_sparse_) va zich (_dense_). Biroq, ushbu kitobda atamalarga asoslangan va _embedding_'ga asoslangan tasniflash tanlandi.
>
> Siyrak qidiruvchilar ma'lumotlarni siyrak vektorlar yordamida ifodalaydi. Siyrak vektor — bu qiymatlarining aksariyati 0 dan iborat bo'lgan vektordir. Atamalarga asoslangan qidiruv siyrak hisoblanadi, chunki har bir atamani siyrak _one-hot_ vektor (faqat bitta qiymati 1, qolgan hammasi 0 bo'lgan vektor) yordamida ifodalash mumkin. Vektor o'lchami lug'at uzunligiga teng bo'ladi. 1 qiymati lug'atdagi atama indeksiga mos keladigan indeksda joylashadi.
>
> Agar bizda `{"food": 0, "banana": 1, "slug": 2}` ko'rinishidagi oddiy lug'at bo'lsa, u holda "food", "banana" va "slug" so'zlarining _one-hot_ vektorlari mos ravishda `[1, 0, 0]`, `[0, 1, 0]` va `[0, 0, 1]` bo'ladi.
>
> Zich qidiruvchilar ma'lumotlarni zich vektorlar yordamida ifodalaydi. Zich vektor — bu qiymatlarining aksariyati 0 bo'lmagan vektordir. _Embedding_'ga asoslangan qidiruv odatda zich hisoblanadi, chunki _embedding_'lar asosan zich vektorlardir. Biroq, siyrak _embedding_'lar ham mavjud. Misol uchun, `SPLADE` (_Sparse Lexical and Expansion_) — bu siyrak _embedding_'lar yordamida ishlaydigan qidiruv algoritmidir ([Formal va boshqalar, 2021](https://arxiv.org/abs/2107.05720)). U `BERT` tomonidan yaratilgan _embedding_'lardan foydalanadi, lekin ko'pchilik _embedding_ qiymatlarini 0 ga tushirish uchun regulyarizatsiyani qo'llaydi. Siyraklik _embedding_ amallarini samaraliroq qiladi.
>
> Siyrak va zich bo'linishi `SPLADE`'ni atamalarga asoslangan algoritmlar bilan bir guruhga tushib qolishiga sabab bo'ladi, garchi `SPLADE`'ning ishlash tamoyillari, kuchli va zaif tomonlari atamalarga asoslangan qidiruvdan ko'ra ko'proq zich _embedding_ qidiruviga o'xshash bo'lsa-da. Atamalarga asoslangan va _embedding_'ga asoslangan tasniflash esa bu noto'g'ri guruhlashning oldini oladi.

### Atamalarga asoslangan qidiruv

So'rov berilganda, unga aloqador hujjatlarni topishning eng to'g'ridan-to'g'ri yo'li — bu kalit so'zlardan foydalanishdir. Ba'zilar bu yondashuvni **leksik qidiruv** (_lexical retrieval_) deb ham atashadi. Masalan, "SI muhandisligi" degan so'rov berilsa, model tarkibida "SI muhandisligi" iborasi qatnashgan barcha hujjatlarni qidirib topadi. Biroq, bu yondashuvda ikkita muammo mavjud:

- Berilgan atama juda ko'p hujjatlarda uchrashi mumkin va modelingizda ularning barchasini kontekst sifatida qamrab olish uchun yetarli joy bo'lmasligi mumkin. Bu o'rinda qo'llaniladigan evristika — atama eng ko'p marta takrorlangan hujjatlarni tanlab olishdir. Bunda, atama hujjatda qancha ko'p uchrasa, ushbu hujjat shu atamaga shunchalik aloqador degan taxminga asoslaniladi. Atamaning hujjatda necha marta uchrashishi **atama chastotasi** (_term frequency — TF_) deb ataladi.

- Prompt uzun bo'lishi va ko'plab atamalarni o'z ichiga olishi mumkin. Ularning ba'zilari boshqalariga qaraganda muhimroqdir. Masalan, "Easy-to-follow recipes for Vietnamese food to cook at home" (Uyda tayyorlash uchun oson vetnamcha taom retseptlari) prompti to'qqizta atamani o'z ichiga oladi: _easy-to-follow, recipes, for, vietnamese, food, to, cook, at, home_. Siz bu yerda _for_ (uchun) va _at_ (-da) kabi yordamchi so'zlarga emas, balki _vietnamese_ (vetnamcha) va _recipes_ (retseptlar) kabi axborotga boy atamalarga e'tibor qaratishni xohlaysiz. Demak, sizga muhim atamalarni aniqlab olish usuli kerak bo'ladi. <br/>
Intuitiv qoida shundaki, atama qancha ko'p hujjatda uchrasa, u shunchalik kam axborot tashiydi. "For" va "at" kabi so'zlar aksariyat hujjatlarda uchrashi tabiiy, shuning uchun ular kamroq ma'lumot beradi. Demak, atamaning muhimligi u qatnashgan hujjatlar soniga teskari proporsionaldir. Ushbu metrika **hujjatning teskari chastotasi** (_inverse document frequency — IDF_) deb ataladi. Biror atama uchun _IDF_'ni hisoblashda ushbu atama qatnashgan barcha hujjatlar sanaladi, so'ngra hujjatlarning umumiy soni shu sanoqqa bo'linadi. Agar jami 10 ta hujjat bo'lib, ulardan 5 tasida berilgan atama mavjud bo'lsa, u holda bu atamaning _IDF_ qiymati $10 / 5 = 2$ ga teng bo'ladi. Atamaning _IDF_ qiymati qancha yuqori bo'lsa, u shunchalik muhim hisoblanadi.

_TF-IDF_ — bu ikki metrikani: atama chastotasi (_TF_) va hujjatning teskari chastotasini (_IDF_) o'zida birlashtirgan algoritmdir. Matematik jihatdan, $D$ hujjati uchun $Q$ so'rovi bo'yicha _TF-IDF_ bali quyidagicha hisoblanadi:

- Aytaylik, $t_1, t_2, ..., t_k$ — $Q$ so'rovidagi atamalar bo'lsin.
- Berilgan $t$ atama uchun, ushbu atamaning $D$ hujjatidagi chastotasi $f(t, D)$ ga teng.
- $N$ — hujjatlarning umumiy soni, $C(t)$ esa $t$ atamasini o'z ichiga olgan hujjatlar soni bo'lsin. $t$ atamasining _IDF_ qiymatini quyidagicha yozish mumkin: $$ IDF(t) = log\frac{N}{C(t)} $$
- Soddalashtirilgan holda, $D$ hujjatining $Q$ so'roviga nisbatan _TF-IDF_ bali quyidagicha aniqlanadi: 

$$(D, Q) = \sum_{i=1}^{q} IDF(t_i) \cdot f(t_i, D)$$

Atamalarga asoslangan qidiruvning ikkita keng tarqalgan yechimi — bu Elasticsearch va `BM25`. [Lucene](https://github.com/apache/lucene) asosida qurilgan [Elasticsearch](https://github.com/elastic/elasticsearch) (Shay Banon, 2010) **teskari indeks** (_inverted index_) deb ataluvchi ma'lumotlar tuzilmasidan foydalanadi. Bu atamalarni ular qatnashgan hujjatlarga bog'laydigan lug'atdir. Ushbu lug'at atama bo'yicha hujjatlarni tezkor qidirib topish imkonini beradi. Indeks, shuningdek, atama chastotasi va hujjatlar sanog'i (bu atama nechta hujjatda borligi) kabi qo'shimcha ma'lumotlarni ham saqlashi mumkin, bu esa _TF-IDF_ ballarini hisoblashda asqotadi. 6-1-jadvalda teskari indeks tasvirlangan.

| Atama | Hujjatlar soni | (Hujjat indeksi, atama chastotasi) ushbu atamani o'z ichiga olgan barcha hujjatlar uchun |
| :--- | :--- | :--- |
| banana | 2 | (10, 3), (5, 2) |
| machine | 4 | (1, 5), (10, 1), (38, 9), (42, 5) |
| learning | 3 | (1, 5), (38, 7), (42, 5) |
| ... | ... | ... |

<div className='text-center text-sm italic'>6-1-jadval. Teskari indeksning soddalashtirilgan misoli.</div>

[`Okapi BM25`](https://en.wikipedia.org/wiki/Okapi_BM25) ("Best Matching" algoritmining 25-avlodi) 1980-yillarda Robertson va boshqalar tomonidan ishlab chiqilgan. Uning baholash tizimi _TF-IDF_'ning modifikatsiyasidir. Oddiy _TF-IDF_ bilan taqqoslaganda, `BM25` atama chastotasi ballarini hujjat uzunligiga qarab normallashtiradi. Uzunroq hujjatlar ma'lum bir atamani o'z ichiga olish ehtimoli yuqoriroq va tabiiy ravishda yuqori atama chastotasi qiymatlariga ega bo'ladi.[^6]

`BM25` va uning variatsiyalari (`BM25+`, `BM25F`) sanoatda hamon keng qo'llaniladi va keyingi bo'limda muhokama qilinadigan _embedding_'ga asoslangan qidiruv kabi zamonaviy, murakkabroq algoritmlarni taqqoslash uchun kuchli tayanch nuqtasi (_baseline_) bo'lib xizmat qiladi.[^7]

Men yuqorida batafsil to'xtalmagan jarayonlardan biri bu — **tokenizatsiya** (_tokenization_), ya'ni so'rovni alohida atamalarga ajratish jarayonidir. Eng oddiy usul — so'rovni so'zlarga bo'lish va har bir so'zga alohida atama sifatida qarashdir. Biroq, bu ko'p so'zli atamalarning alohida so'zlarga bo'linib ketishiga va asl ma'nosini yo'qotishiga olib kelishi mumkin. Masalan, "hot dog" atamasi "hot" (issiq) va "dog" (it) so'zlariga ajratilishi mumkin. Bunday holda, ularning hech biri asl atamaning ma'nosini saqlab qolmaydi. Bu muammoni yumshatishning bir yo'li — eng keng tarqalgan _n-gram_'larga atama sifatida qarashdir. Agar "hot dog" bigrammasi (ikki so'zli birikma) keng tarqalgan bo'lsa, unga yagona atama sifatida munosabatda bo'linadi.

Qo'shimchasiga, siz barcha belgilarni kichik harflarga o'tkazishni, tinish belgilarini olib tashlashni va stop-so'zlarni ("the", "and", "is" va h.k.) chiqarib tashlashni xohlashingiz mumkin. Atamalarga asoslangan qidiruv yechimlari ko'pincha bularni avtomatik tarzda bajaradi. [NLTK](https://www.nltk.org/) (Natural Language Toolkit), [spaCy](https://github.com/explosion/spaCy) va [Stanford CoreNLP](https://github.com/stanfordnlp/CoreNLP) kabi klassik _NLP_ paketlari ham tokenizatsiya funksiyalarini taklif etadi.

4-bobda ikki matn o'rtasidagi leksik o'xshashlikni ularning _n-gram_ kesishuviga asoslanib o'lchash muhokama qilingan edi. Hujjatlarni ularning so'rov bilan _n-gram_ kesishuvi darajasiga qarab qidirib topa olamizmi? Ha, albatta. Bu yondashuv so'rov va hujjatlar o'xshash uzunlikka ega bo'lganda eng yaxshi natija beradi. Agar hujjatlar so'rovdan ancha uzun bo'lsa, ularda so'rovdagi _n-gram_'larning uchrash ehtimoli ortadi, bu esa ko'plab hujjatlarning bir xil darajadagi yuqori o'xshashlik ballariga ega bo'lishiga olib keladi. Bu esa haqiqatdan ham aloqador hujjatlarni kamroq aloqadorlaridan ajratib olishni qiyinlashtiradi.

### _Embedding_'ga asoslangan qidiruv

Atamalarga asoslangan qidiruv aloqadorlikni semantik emas, balki leksik darajada hisoblaydi. 3-bobda ta'kidlanganidek, matnning tashqi ko'rinishi har doim ham uning ma'nosini to'liq aks ettirmaydi. Bu esa maqsadingizga aloqasi bo'lmagan hujjatlarning qaytarilishiga olib kelishi mumkin. Masalan, "transformer arxitekturasi" deb so'rov berish elektr qurilmasi yoki "Transformers" filmi haqidagi hujjatlarni qaytarishi mumkin. Aksincha, _embedding_'ga asoslangan qidiruvchilar hujjatlarni ularning ma'nosi so'rovga qanchalik yaqinligiga qarab darajalashni maqsad qiladi. Bu yondashuv **semantik qidiruv** deb ham ataladi.

_Embedding_'ga asoslangan qidiruvda indekslash jarayoni qo'shimcha vazifaga ega: asl ma'lumot bo'laklarini _embedding_'larga aylantirish. Yaratilgan _embedding_'lar saqlanadigan baza **vektorli ma'lumotlar bazasi** deb ataladi. So'rov yuborish jarayoni esa, 6-3-rasmda ko'rsatilganidek, ikki bosqichdan iborat:

1.  **_Embedding_ modeli:** so'rovni indekslash paytida ishlatilgan ayni o'sha _embedding_ modeli yordamida _embedding_'ga aylantiradi.
2.  **Qidiruvchi:** _embedding_'lari so'rov _embedding_'iga eng yaqin bo'lgan $k$ ta ma'lumot bo'lagini olib keladi (bu yaqinlik qidiruvchi tomonidan aniqlanadi). Olib kelinadigan ma'lumot bo'laklari soni — $k$, ishlatilish senariysi, generativ model va so'rovga bog'liq.

![6-3-rasm. Embedding'ga asoslangan yoki semantik qidiruvchi qanday ishlashining yuqori darajadagi ko'rinishi.](/ai-engineering/6-chapter/6.3-figure.png)

<div className='text-center text-sm italic'>6-3-rasm. Embedding'ga asoslangan yoki semantik qidiruvchi qanday ishlashining yuqori darajadagi ko'rinishi.</div>

Bu yerda ko'rsatilgan _embedding_'ga asoslangan qidiruv jarayoni soddalashtirilgan. Real hayotdagi semantik qidiruv tizimlari barcha topilgan nomzodlarni qayta darajalash uchun **qayta darajalovchi** (_reranker_) va kechikishni kamaytirish uchun keshlar kabi boshqa komponentlarni ham o'z ichiga olishi mumkin.[^8]

_Embedding_'ga asoslangan qidiruv bilan biz yana 3-bobda muhokama qilingan _embedding_'larga duch kelamiz. Eslatib o'tamiz, _embedding_ — bu odatda asl ma'lumotlarning muhim xususiyatlarini saqlab qolishga qaratilgan vektor. Agar _embedding_ modeli yomon bo'lsa, _embedding_'ga asoslangan qidiruvchi ishlamaydi.

_Embedding_'ga asoslangan qidiruv, shuningdek, yangi komponenti — vektorli ma'lumotlar bazalarini ham joriy etadi. Vektorli ma'lumotlar bazasi vektorlarni saqlaydi. Biroq, saqlash — bu vektorli bazaning eng oson qismi. Qiyin qismi — bu **vektorli qidiruv**. So'rov _embedding_'i berilganda, vektorli baza o'zidagi so'rovga yaqin vektorlarni topish va qaytarish uchun javobgardir. Vektorli qidiruv tez va samarali bo'lishi uchun vektorlar maxsus usulda indekslanishi va saqlanishi kerak.

Generativ _SI_ ilovalari tayanadigan boshqa ko'plab mexanizmlar singari, vektorli qidiruv ham faqat generativ _SI_'ga xos emas. Vektorli qidiruv _embedding_'lardan foydalanadigan har qanday ilovada: qidiruv, tavsiya tizimlari, ma'lumotlarni tashkil etish, axborot qidiruvi, klasterlash, firibgarlikni aniqlash va boshqalarda keng tarqalgan.

Vektorli qidiruv odatda **eng yaqin qo'shnini qidirish** muammosi sifatida qaraladi. Masalan, so'rov berilganda, $k$ ta eng yaqin vektorni topish talab etiladi. Buning eng sodda yechimi — **$k$-eng yaqin qo'shnilar** ($k$-NN) bo'lib, u quyidagicha ishlaydi:

1.  Kosinus o'xshashligi kabi metrikalar yordamida so'rov _embedding_'i va bazadagi barcha vektorlar o'rtasidagi o'xshashlik ballarini hisoblash.
2.  Barcha vektorlarni o'xshashlik ballari bo'yicha darajalash.
3.  Eng yuqori o'xshashlik balliga ega $k$ ta vektorni qaytarish.

Ushbu sodda yechim natijalarning aniq bo'lishini ta'minlaydi, ammo u hisoblash jihatidan og'ir va sekin ishlaydi. Undan faqat kichik ma'lumotlar to'plamlari uchun foydalanish maqsadga muvofiq.

Katta to'plamlar uchun vektorli qidiruv odatda **taxminiy eng yaqin qo'shni** (_ANN_) algoritmi yordamida amalga oshiriladi. Vektorli qidiruvning ahamiyati yuqoriligi sababli, u uchun ko'plab algoritm va kutubxonalar ishlab chiqilgan. Mashhur vektorli qidiruv kutubxonalariga `FAISS` (Facebook AI Similarity Search) ([Johnson va boshqalar, 2017](https://arxiv.org/abs/1702.08734)), Google'ning `ScaNN` (Scalable Nearest Neighbors) ([Sun va boshqalar, 2020](https://research.google/blog/announcing-scann-efficient-vector-similarity-search/)), [Spotify'ning `Annoy`](https://github.com/spotify/annoy) (Bernhardsson, 2013) va [`Hnswlib`](https://github.com/nmslib/hnswlib) ([Hierarchical Navigable Small World](https://github.com/nmslib/hnswlib)) (Malkov va Yashunin, 2016) kiradi.

Aksariyat ilova dasturchilari vektorli qidiruvni o'zlari noldan amalga oshirmaydilar, shuning uchun men turli yondashuvlar haqida faqat qisqacha umumiy ma'lumot berib o'taman. Bu ma'lumotlar yechimlarni baholash jarayonida sizga asqotishi mumkin.

Umuman olganda, vektorli ma'lumotlar bazalari vektorlarni savatchalarga (_buckets_), daraxtlarga yoki graflarga ajratadi. Vektorli qidiruv algoritmlari o'xshash vektorlarning bir-biriga yaqin joylashish ehtimolini oshirish uchun qo'llaydigan evristikalariga ko'ra farqlanadi. Vektorlar, shuningdek, kvantlanishi (aniqligi kamaytirilishi) yoki siyraklashtirilishi mumkin. Bundan ko'zlangan maqsad shuki, kvantlangan va siyrak vektorlar bilan ishlash hisoblash jihatidan kamroq resurs talab qiladi. Vektorli qidiruvni chuqurroq o'rganishni istaganlar uchun Zilliz kompaniyasi bu mavzuda ajoyib [maqolalar seriyasini](https://zilliz.com/learn/vector-index) taqdim etgan. Quyida ba'zi muhim vektorli qidiruv algoritmlari keltirilgan:

- ***`LSH` (Locality-sensitive hashing — Joylashuvga sezgir xeshlashtirish) ([Indyk va Motwani, 1999](https://graphics.stanford.edu/courses/cs468-06-fall/Papers/06%20indyk%20motwani%20-%20stoc98.pdf))***:
Bu nafaqat vektorlar bilan ishlaydigan kuchli va ko'p qirrali algoritmdir. U o'xshashlik qidiruvini tezlashtirish uchun o'xshash vektorlarni bitta savatchaga xeshlashtirishni o'z ichiga oladi, bunda samaradorlik evaziga aniqlikdan biroz voz kechiladi. U `FAISS` va `Annoy` kutubxonalarida joriy etilgan.

- ***`HNSW` (Hierarchical Navigable Small World — Ierarxik navigatsiyalanuvchi kichik dunyo) ([Malkov va Yashunin, 2016](https://github.com/nmslib/hnswlib))***:
`HNSW` ko'p qavatli graf tuzadi, unda tugunlar vektorlarni, qirralar esa o'xshash vektorlarni bog'laydi. Bu graf qirralari bo'ylab harakatlanish orqali eng yaqin qo'shnilarni qidirish imkonini beradi. Mualliflar tomonidan qilingan tatbiq ochiq manbali bo'lib, u shuningdek `FAISS` va `Milvus`'da ham mavjud.

- ***`Product Quantization` (Ko'paytmani kvantlash) ([Jégou va boshqalar, 2011](https://inria.hal.science/inria-00514462v2/document))***:
Bu usul har bir vektorni bir nechta kichik vektorlarga (subvektorlarga) ajratish orqali ancha sodda, quyi o'lchamli ko'rinishga keltirish asosida ishlaydi. Masofalar keyinchalik shu quyi o'lchamli ko'rinishlar yordamida hisoblanadi, bu esa ishlashni ancha tezlashtiradi. `Product Quantization` `FAISS`'ning asosiy komponenti hisoblanadi va deyarli barcha mashhur vektorli qidiruv kutubxonalari tomonidan qo'llab-quvvatlanadi.

- ***`IVF` (Inverted file index — Teskari fayl indeksi) ([Sivic va Zisserman, 2003](https://ieeexplore.ieee.org/abstract/document/1238663))***:
`IVF` o'xshash vektorlarni bitta klasterga yig'ish uchun `K-means` klasterlashdan foydalanadi. Bazadagi vektorlar soniga qarab, har bir klasterda o'rtacha 100 dan 10 000 gacha vektor bo'lishi belgilanadi. So'rov paytida `IVF` so'rov _embedding_'iga eng yaqin klaster markazlarini (centroid) topadi va shu klasterlardagi vektorlar nomzod qo'shnilarga aylanadi. `Product Quantization` bilan birgalikda `IVF` `FAISS`'ning o'zagini tashkil qiladi.

- ***`Annoy` (Approximate Nearest Neighbors Oh Yeah) ([Bernhardsson, 2013](https://github.com/spotify/annoy))***:
`Annoy` — daraxtga asoslangan yondashuv. U bir nechta binar daraxtlarni quradi, har bir daraxt tasodifiy mezonlar (masalan, tasodifiy chiziq tortish va vektorlarni shu chiziq bo'yicha ikki tarmoqqa ajratish) asosida vektorlarni klasterlarga bo'ladi. Qidiruv paytida nomzod qo'shnilarni yig'ish uchun shu daraxtlar bo'ylab harakatlaniladi. Spotify o'z tatbiqini ochiq manbaga chiqargan.

Bulardan tashqari Microsoft'ning `SPTAG` (_Space Partition Tree And Graph_) va `FLANN` (_Fast Library for Approximate Nearest Neighbors_) kabi boshqa algoritmlar ham mavjud.

Garchi vektorli ma'lumotlar bazalari _RAG_'ning yuksalishi bilan alohida toifa sifatida paydo bo'lgan bo'lsa-da, vektorlarni saqlay oladigan har qanday bazani vektorli baza deyish mumkin. Ko'plab an'anaviy ma'lumotlar bazalari vektor saqlash va vektorli qidiruv imkoniyatlarini o'zlariga qo'shgan yoki kelajakda qo'shishni rejalashtirmoqda.

### Qidiruv algoritmlarini taqqoslash

Qidiruvning uzoq tarixi tufayli, uning ko'plab yetuk yechimlari ham atamalarga asoslangan, ham _embedding_'ga asoslangan qidiruvni boshlashni nisbatan osonlashtiradi. Har bir yondashuvning o'z yutuq va kamchiliklari bor.

Atamalarga asoslangan qidiruv odatda indekslash va so'rov paytida _embedding_'ga asoslangan qidiruvdan ancha tezroq ishlaydi. Atamalarni ajratib olish _embedding_ generatsiyasidan tezroq, atamani u qatnashgan hujjatlarga bog'lash esa eng yaqin qo'shnini qidirishdan ko'ra kamroq hisoblash resursini talab qiladi.

Atamalarga asoslangan qidiruv, shuningdek, qo'shimcha sozlamalarsiz ham yaxshi ishlaydi. `Elasticsearch` va `BM25` kabi yechimlar ko'plab qidiruv va axborot olish ilovalarini muvaffaqiyatli quvvatlab kelmoqda. Biroq, uning soddaligi shuni anglatadiki, uning ishlash samaradorligini oshirish uchun o'zgartirishingiz mumkin bo'lgan komponentlar soni kamroq.

Boshqa tomondan, _embedding_'ga asoslangan qidiruv vaqt o'tishi bilan atamalarga asoslangan qidiruvdan o'tib ketishi uchun sezilarli darajada takomillashtirilishi mumkin. Siz _embedding_ modelini va qidiruvchini alohida, birga yoki generativ model bilan hamkorlikda _finetuning_ qilishingiz mumkin. Biroq, ma'lumotlarni _embedding_'ga aylantirish jarayoni aniq kalit so'zlarni, masalan, `EADDRNOTAVAIL` (99) kabi maxsus xatolik kodlarini yoki mahsulot nomlarini xiralashtirib yuborishi mumkin, bu esa ularni keyinchalik qidirishni qiyinlashtiradi. Bu cheklovni ushbu bobda keyinroq muhokama qilinadigan _embedding_'ga asoslangan qidiruvni atamalarga asoslangan qidiruv bilan birlashtirish orqali hal qilish mumkin.

Qidiruvchining sifati u topgan ma'lumotlarning sifatiga qarab baholanishi mumkin. _RAG_ baholash freymvorklari tomonidan tez-tez ishlatiladigan ikkita metrika — bu **kontekst aniqligi** (_context precision_) va **kontekst qamrovi** (_context recall_), yoki qisqacha aniqlik va qamrov (kontekst aniqligi ba'zan kontekst aloqadorligi deb ham ataladi):

- **Kontekst aniqligi**:
Qidirib topilgan barcha hujjatlarning necha foizi so'rovga aloqador?

- **Kontekst qamrovi**:
So'rovga aloqador bo'lgan barcha hujjatlarning necha foizi qidirib topildi?

Ushbu metrikalarni hisoblash uchun siz test so'rovlari ro'yxati va hujjatlar to'plamidan iborat baholash to'plamini shakllantirasiz. Har bir test so'rovi uchun har bir test hujjatini aloqador yoki aloqador emas deb belgilab chiqasiz. Belgilash jarayoni insonlar yoki _SI_-baholovchilar tomonidan amalga oshirilishi mumkin. So'ngra ushbu baholash to'plamida qidiruvchining aniqlik va qamrov ballarini hisoblaysiz.

Real loyihalarda (_production_) ba'zi _RAG_ freymvorklari faqat kontekst aniqligini qo'llab-quvvatlaydi, kontekst qamrovini emas. Muayyan so'rov uchun kontekst qamrovini hisoblash maqsadida siz bazangizdagi barcha hujjatlarning ushbu so'rovga aloqadorligini belgilab chiqishingiz kerak bo'ladi. Kontekst aniqligini hisoblash esa osonroq. Buning uchun faqat qidirib topilgan hujjatlarni so'rov bilan solishtirish kifoya, buni esa _SI_-baholovchi amalga oshirishi mumkin.

Agar siz uchun qidirib topilgan hujjatlarning darajalanish tartibi muhim bo'lsa (masalan, eng aloqador hujjatlar birinchi bo'lib chiqishi kerak bo'lsa), [`NDCG`](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) (_Normalized Discounted Cumulative Gain_), [`MAP`](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision) (_Mean Average Precision_) va [`MRR`](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) (_Mean Reciprocal Rank_) kabi metrikalardan foydalanishingiz mumkin.

Semantik qidiruv uchun _embedding_'laringiz sifatini ham baholashingiz kerak. 3-bobda muhokama qilinganidek, _embedding_'lar mustaqil ravishda baholanishi mumkin — agar o'xshashroq hujjatlar yaqinroq _embedding_'larga ega bo'lsa, ular yaxshi hisoblanadi. _embedding_'lar, shuningdek, aniq vazifalar uchun qanchalik yaxshi ishlashiga qarab ham baholanishi mumkin. [`MTEB`](https://arxiv.org/abs/2210.07316) benchmarki (Muennighoff va boshqalar, 2023) _embedding_'larni qidiruv, tasniflash va klasterlash kabi keng ko'lamli vazifalar bo'yicha baholaydi.

Qidiruvchining sifati butun _RAG_ tizimi kontekstida ham baholanishi lozim. Pirovardida, agar qidiruvchi tizimga yuqori sifatli javoblar yaratishda yordam bersa, u yaxshi hisoblanadi. Generativ modellarning natijalarini baholash 3 va 4-boblarda muhokama qilingan.

Semantik qidiruv tizimi va'da qilayotgan samaradorlik uning ortidan quvishga arziydimi yoki yo'q — bu sizning xarajat va kechikishni qanchalik ustuvor deb bilishingizga, ayniqsa so'rov yuborish bosqichida bunga qanchalik e'tibor berishingizga bog'liq. _RAG_ kechikishining katta qismi javobni generatsiya qilishdan (ayniqsa uzun javoblar uchun) kelib chiqqani sababli, so'rov _embedding_'ini yaratish va vektorli qidiruv qo'shadigan kechikish umumiy _RAG_ vaqtiga nisbatan arzimas bo'lishi mumkin. Shunday bo'lsa-da, qo'shimcha kechikish foydalanuvchi tajribasiga ta'sir qilishi mumkin.

Yana bir xavotirli jihat — bu xarajat. _Embedding_ yaratish pul turadi. Agar ma'lumotlaringiz tez-tez o'zgarib tursa va _embedding_'larni tez-tez qayta yaratishni talab qilsa, bu jiddiy muammoga aylanishi mumkin. Har kuni 100 millionta hujjat uchun _embedding_ yaratish kerakligini tasavvur qiling! Qaysi vektorli ma'lumotlar bazasidan foydalanishingizga qarab, vektorlarni saqlash va vektorli qidiruv so'rovlari ham qimmatga tushishi mumkin. Kompaniyaning vektorli ma'lumotlar bazasiga sarflaydigan xarajatlari model _API_'lariga sarflanadigan mablag'ning beshdan birini yoki hatto yarmini tashkil etishi kamyob holat emas.

6-2-jadvalda atamalarga asoslangan qidiruv va _embedding_'ga asoslangan qidiruvning yonma-yon taqqoslamasi keltirilgan.

| | Atamalarga asoslangan qidiruv | _Embedding_'ga asoslangan qidiruv |
| :--- | :--- | :--- |
| **So'rov tezligi** | _Embedding_'ga asoslangan qidiruvdan ancha tezroq | So'rov _embedding_'ini yaratish va vektorli qidiruv sekin bo'lishi mumkin |
| **Samaradorlik** | Odatda qo'shimcha sozlamalarsiz ham kuchli natija beradi, lekin yaxshilash qiyin <br/><br/> Atamalarning ko'p ma'noliligi sababli noto'g'ri hujjatlarni qaytarishi mumkin | _Finetuning_ yordamida atamalarga asoslangan qidiruvdan o'tib ketishi mumkin <br/><br/> Tabiiyroq so'rovlardan foydalanish imkonini beradi, chunki u atamalarga emas, semantikaga e'tibor qaratadi |
| **Xarajat** | _Embedding_'ga asoslangan qidiruvdan ancha arzon | _Embedding_, vektorlarni saqlash va vektorli qidiruv yechimlari qimmat bo'lishi mumkin |

<div className='text-center text-sm italic'>6-2-jadval. Atamalarga asoslangan qidiruv va semantik qidiruvning tezlik, samaradorlik va xarajat bo'yicha taqqoslamasi.</div>

Qidiruv tizimlari bilan ishlashda siz indekslash va so'rov yuborish o'rtasida ma'lum murosalarga (_trade-offs_) borishingiz mumkin. Indeks qanchalik batafsil bo'lsa, qidiruv jarayoni shunchalik aniq bo'ladi, lekin indekslash jarayoni sekinlashadi va ko'proq xotira talab qiladi. Tasavvur qiling, siz potensial mijozlar indeksini tuzayapsiz. Ko'proq ma'lumotlarni (masalan, ism, kompaniya, elektron pochta, telefon, qiziqishlar) qo'shish kerakli odamlarni topishni osonlashtiradi, lekin bunday indeksni tuzish ko'proq vaqt oladi va ko'proq joy talab qiladi.

Umuman olganda, `HNSW` kabi batafsil indeks yuqori aniqlik va tezkor so'rov vaqtlarini ta'minlaydi, ammo uni qurish uchun sezilarli vaqt va xotira talab etiladi. Aksincha, `LSH` kabi soddaroq indeks tezroq va kamroq xotira bilan yaratiladi, lekin bu sekinroq va kamroq aniqlikdagi so'rovlarga olib keladi.

[ANN-Benchmarks veb-sayti](https://ann-benchmarks.com/index.html) turli _ANN_ algoritmlarini indekslash va so'rov yuborish o'rtasidagi murosalarni hisobga olgan holda, to'rtta asosiy metrika yordamida bir nechta ma'lumotlar to'plamlarida taqqoslaydi. Bular quyidagilardir:

- **Qamrov** (_Recall_): Algoritm tomonidan topilgan eng yaqin qo'shnilarning ulushi.

- **Sekundiga so'rovlar soni** (_QPS_): Algoritm bir soniyada qayta ishlay oladigan so'rovlar soni. Bu yuqori yuklamali ilovalar uchun juda muhimdir.

- **Qurish vaqti** (_Build time_): Indeksni qurish uchun talab qilinadigan vaqt. Agar siz indeksni tez-tez yangilab turishingiz kerak bo'lsa (masalan, ma'lumotlaringiz o'zgarib tursa), bu metrika ayniqsa muhimdir.

- **Indeks hajmi** (_Index size_): Algoritm tomonidan yaratilgan indeksning o'lchami. Bu uning masshtablash imkoniyati va saqlash talablarini baholashda hal qiluvchi ahamiyatga ega.

Qo'shimcha ravishda, `BEIR` (_Benchmarking IR_) ([Thakur va boshqalar, 2021](https://arxiv.org/abs/2104.08663)) qidiruv tizimlari uchun baholash vositasidir. U 14 ta keng tarqalgan qidiruv benchmarklari bo'ylab qidiruv tizimlarini qo'llab-quvvatlaydi.

Xulosa qilib aytganda, _RAG_ tizimining sifati ham komponentma-komponent, ham boshdan-oyoq baholanishi kerak. Buning uchun siz quyidagi ishlarni amalga oshirishingiz lozim:

1. Qidiruv sifatini baholash.
2. Yakuniy _RAG_ natijalarini baholash.
3. _Embedding_'larni baholash (_embedding_'ga asoslangan qidiruv uchun).

### Qidiruv algoritmlarini birlashtirish

Turli qidiruv algoritmlarining o'ziga xos ustunliklarini inobatga olgan holda, real loyihalardagi qidiruv tizimlari odatda bir nechta yondashuvlarni birlashtiradi. Atamalarga asoslangan qidiruv va _embedding_'ga asoslangan qidiruvni birlashtirish **gibrid qidiruv** deb ataladi.

Turli algoritmlar ketma-ketlikda qo'llanilishi mumkin. Avvaliga, atamalarga asoslangan tizim kabi arzon va kamroq aniqlikdagi qidiruvchi nomzodlarni yig'ib keladi. So'ngra, $k$-eng yaqin qo'shnilar kabi aniqroq, ammo qimmatroq mexanizm ushbu nomzodlar orasidan eng yaxshilarini topadi. Bu ikkinchi bosqich **qayta darajalash** (_reranking_) deb ham ataladi.

Masalan, "transformer" atamasi berilganda, siz ular elektr qurilmasi, neyron arxitektura yoki kino haqida ekanligidan qat'i nazar, tarkibida "transformer" so'zi qatnashgan barcha hujjatlarni chaqirib olishingiz mumkin. Keyin esa, ushbu hujjatlar orasidan aynan sizning "transformer" so'rovingizga aloqador bo'lganlarini topish uchun vektorli qidiruvdan foydalanasiz. Yana bir misol sifatida "X ga qilingan eng ko'p savdo uchun kim javobgar?" degan so'rovni olaylik. Avvaliga siz X kalit so'zidan foydalanib, X bilan bog'liq barcha hujjatlarni topishingiz mumkin. So'ngra, "Eng ko'p savdo uchun kim javobgar?" mazmuniga aloqador kontekstni qidirib topish uchun vektorli qidiruvni ishga solasiz.

Turli algoritmlar parallel ravishda, **ansambl** sifatida ham ishlatilishi mumkin. Eslatib o'tamiz, qidiruvchi hujjatlarni so'rovga aloqadorlik ballari bo'yicha darajalash orqali ishlaydi. Siz bir vaqtning o'zida nomzodlarni olish uchun bir nechta qidiruvchilardan foydalanishingiz, so'ngra yakuniy darajalashni hosil qilish uchun ushbu turli natijalarni birlashtirishingiz mumkin.

Turli darajalashlarni birlashtirish algoritmi [**teskari darajalarni birlashtirish**](https://cormack.uwaterloo.ca/cormacksigir09-rrf.pdf) (_RRF_ — _Reciprocal Rank Fusion_) deb ataladi (Cormack va boshqalar, 2009). U har bir hujjatga qidiruvchi tomonidan belgilangan o'rniga (darajasiga) qarab ball beradi. Mantiqan olganda, agar u birinchi o'rinda tursa, uning bali $1/1 = 1$ bo'ladi. Agar ikkinchi o'rinda tursa, bali $1/2 = 0.5$ bo'ladi. U qancha yuqori o'rinda tursa, bali shuncha yuqori bo'ladi.

Hujjatning yakuniy bali uning barcha qidiruvchilardagi ballari yig'indisiga teng. Agar hujjat bir qidiruvchi tomonidan birinchi, boshqasi tomonidan ikkinchi o'ringa qo'yilgan bo'lsa, uning bali $1 + 0.5 = 1.5$ bo'ladi. Bu misol _RRF_'ning o'ta soddalashtirilgan ko'rinishidir, ammo u asosiy mohiyatni ochib beradi. $D$ hujjati uchun haqiqiy formula murakkabroq va quyidagicha ko'rinishga ega:

$$ score(D) = \sum_{i=1}^{n} \frac{1}{k + r_i(D)} $$

Bu yerda:
- $n$ — darajalangan ro'yxatlar soni; har bir ro'yxat bitta qidiruvchi tomonidan yaratiladi.
- $r_i(D)$ — $i$-qidiruvchi bo'yicha hujjatning o'rni (darajasi).
- $k$ — nolga bo'lishning oldini olish va quyi o'rinlardagi hujjatlarning ta'sirini nazorat qilish uchun o'zgarmas son (konstanta). $k$ uchun tipik qiymat — 60.

## Qidiruvni optimallashtirish

Vazifaga qarab, ma'lum taktikalar tegishli hujjatlarning qidirib topilish imkoniyatini oshirishi mumkin. Bu yerda muhokama qilinadigan to'rtta taktika: **bo'laklash strategiyasi** (_chunking strategy_), **qayta darajalash** (_reranking_), **so'rovni qayta yozish** (_query rewriting_) va **kontekstual qidiruv** (_contextual retrieval_).

### Bo'laklash strategiyasi

Ma'lumotlaringiz qanday indekslanishi ularni keyinchalik qanday qidirib olishni rejalashtirayotganingizga bog'liq. O'tgan bo'limda turli xil qidiruv algoritmlari va ularga mos indekslash strategiyalari yoritildi. U yerdagi muhokama hujjatlar allaqachon boshqarish oson bo'lgan bo'laklarga ajratilgan degan farazga asoslangan edi. Ushbu bo'limda men turli xil **bo'laklash** (_chunking_) strategiyalarini ko'rib chiqaman. Bu juda muhim masala, chunki siz qo'llaydigan bo'laklash strategiyasi qidiruv tizimingiz samaradorligiga sezilarli ta'sir ko'rsatishi mumkin.

Eng oddiy strategiya — hujjatlarni ma'lum bir birlik asosida teng uzunlikdagi bo'laklarga ajratishdir. Keng tarqalgan birliklar — bu belgilar, so'zlar, gaplar va paragraflardir. Masalan, siz har bir hujjatni 2,048 belgi yoki 512 so'zdan iborat bo'laklarga bo'lishingiz mumkin. Shuningdek, har bir hujjatni shunday bo'lishingiz mumkinki, har bir bo'lak belgilangan sondagi gaplarni (masalan, 20 ta gap) yoki paragraflarni (masalan, har bir paragraf o'z-o'zidan bitta bo'lak) o'z ichiga olsin.

Hujjatlarni, har bir bo'lak sizning maksimal bo'lak o'lchamingizga sig'maguncha, tobora kichikroq birliklardan foydalangan holda **rekursiv** tarzda ham bo'lishingiz mumkin. Masalan, hujjatni bo'limlarga ajratishdan boshlaysiz. Agar bo'lim juda uzun bo'lsa, uni paragraflarga bo'lasiz. Agar paragraf hamon juda uzun bo'lsa, uni gaplarga bo'lasiz. Bu o'zaro bog'liq matnlarning tasodifan uzilib qolish ehtimolini kamaytiradi.

Maxsus turdagi hujjatlar ijodiy bo'laklash strategiyalarini talab qilishi mumkin. Masalan, turli dasturlash tillari uchun maxsus ishlab chiqilgan ajratgichlar ([_splitters_](https://github.com/grantjenks/py-tree-sitter-languages#license)) mavjud. Savol-javob hujjatlari savol va javob juftligi bo'yicha bo'linishi mumkin, bunda har bir juftlik bitta bo'lakni tashkil qiladi. Xitoy tili matnlari ingliz tili matnlaridan farqli ravishda bo'laklanishi kerak bo'lishi mumkin.

Agar hujjat bir-biri bilan kesishmaydigan (_without overlap_) bo'laklarga ajratilsa, bo'laklar muhim kontekstning o'rtasidan uzilib qolishi, bu esa muhim ma'lumotlarning yo'qolishiga olib kelishi mumkin. "I left my wife a note" (Men xotinimga xat qoldirdim) matnini ko'rib chiqaylik. Agar u "I left my wife" (Men xotinimni tashlab ketdim) va "a note" (xat) deb bo'linsa, bu ikki bo'lakning hech biri asl matnning asosiy mazmunini yetkazib bermaydi. **Ustma-ust tushish** (_overlapping_) muhim chegara ma'lumotlarining kamida bitta bo'lakka kiritilishini ta'minlaydi. Agar bo'lak o'lchamini 2,048 belgi qilib belgilasangiz, ustma-ust tushish o'lchamini ehtimol 20 belgi qilib olishingiz mumkin.

Bo'lak o'lchami generativ modelning maksimal kontekst uzunligidan oshmasligi kerak. _Embedding_'ga asoslangan yondashuv uchun bo'lak o'lchami _embedding_ modelining kontekst limitidan ham oshmasligi lozim.

Hujjatlarni generativ modelning tokenizatori tomonidan aniqlanadigan **tokenlar** yordamida ham bo'laklashingiz mumkin. Aytaylik, siz generativ model sifatida `Llama 3`'dan foydalanmoqchisiz. Unda siz avval hujjatlarni `Llama 3`'ning tokenizatori yordamida tokenlarga ajratasiz. So'ngra tokenlarni chegara sifatida ishlatib, hujjatlarni bo'laklarga bo'lishingiz mumkin. Tokenlar bo'yicha bo'laklash quyi oqim modellari bilan ishlashni osonlashtiradi. Biroq, bu yondashuvning kamchiligi shundaki, agar siz boshqa tokenizatorga ega bo'lgan boshqa generativ modelga o'tsangiz, ma'lumotlaringizni qayta indekslashingizga to'g'ri keladi.

Qaysi strategiyani tanlashingizdan qat'i nazar, bo'lak o'lchamlari muhim ahamiyatga ega. Kichikroq bo'lak o'lchami xilma-xil ma'lumotlarni olish imkonini beradi. Kichikroq bo'laklar model kontekstiga ko'proq sondagi bo'laklarni sig'dira olishingizni anglatadi. Agar bo'lak o'lchamini ikki baravar kichraytirsangiz, ikki baravar ko'p bo'lakni sig'dira olasiz. Ko'proq bo'laklar modelni kengroq qamrovli ma'lumotlar bilan ta'minlashi mumkin, bu esa modelga yaxshiroq javob yaratish imkonini beradi.

Biroq, kichik bo'lak o'lchamlari muhim ma'lumotlarning yo'qolishiga sabab bo'lishi mumkin. Tasavvur qiling, hujjatda $X$ mavzusi haqida muhim ma'lumotlar bor, lekin $X$ faqat hujjatning birinchi yarmida tilga olingan. Agar siz ushbu hujjatni ikki bo'lakka bo'lsangiz, ikkinchi yarmi qidirib topilganda $X$ haqidagi kontekst tushib qolishi va model uning ma'lumotlaridan foydalana olmasligi mumkin.

Kichik bo'lak o'lchamlari hisoblash yuklamasini (_computational overhead_) ham oshirishi mumkin. Bu, ayniqsa, _embedding_'ga asoslangan qidiruv uchun jiddiy muammodir. Bo'lak o'lchamini ikki baravar kamaytirish — indekslash uchun ikki baravar ko'p bo'lak va yaratish hamda saqlash uchun ikki baravar ko'p _embedding_ vektorlari deganidir. Vektorli qidiruv fazoingiz ikki baravar kattalashadi, bu esa so'rov tezligini pasaytirishi mumkin.

Universal eng yaxshi bo'lak o'lchami yoki ustma-ust tushish o'lchami mavjud emas. Siz o'zingiz uchun nima eng yaxshi ishlashini tajriba qilib topishingiz kerak.

### Qayta darajalash

Qidiruvchi tomonidan taqdim etilgan dastlabki hujjatlar tartibini aniqlikni oshirish maqsadida **qayta darajalash** (_reranking_) mumkin. Qayta darajalash, ayniqsa, model kontekstiga sig'dirish yoki kiruvchi tokenlar sonini qisqartirish maqsadida qidirib topilgan hujjatlar sonini kamaytirish zarur bo'lganda juda qo'l keladi.

Qayta darajalashning keng tarqalgan andozalaridan biri yuqorida keltirilgan ["Qidiruv algoritmlarini birlashtirish"](/books/ai-engineering/6-rag-and-agents/rag#qidiruv-algoritmlarini-birlashtirish) mavzusida  muhokama qilingan edi. Bunda arzon, ammo aniqligi pastroq qidiruvchi dastlabki nomzodlarni yig'ib keladi, so'ngra aniqroq, lekin qimmatroq mexanizm ushbu nomzodlarni qayta darajalab chiqadi.

Hujjatlar vaqt omiliga asoslanib, yangiroq ma'lumotlarga yuqori og'irlik berish orqali ham qayta darajalanishi mumkin. Bu yangiliklar agregatsiyasi, elektron pochta orqali muloqot (masalan, xatlaringiz bo'yicha savollarga javob beradigan chatbot) yoki fond bozori tahlili kabi vaqtga sezgir ilovalar uchun foydalidir.

Kontekstni qayta darajalash an'anaviy qidiruvdagi qayta darajalashdan shu bilan farq qiladiki, unda elementlarning aniq joylashuvi unchalik hal qiluvchi ahamiyatga ega emas. Qidiruv tizimlarida natijaning o'rni (masalan, birinchi yoki beshinchi bo'lib chiqishi) o'ta muhimdir. Kontekstni qayta darajalashda ham hujjatlar tartibi ahamiyatga ega, chunki bu modelning ularni qayta ishlash sifatiga ta'sir ko'rsatadi. ["Kontekst uzunligi va kontekst samaradorligi"](/books/ai-engineering/5-prompt-engineering/introduction-to-prompting#kontekst-hajmi-va-undan-foydalanish-samaradorligi) mavzusida muhokama qilinganidek, modellar kontekstning boshi va oxirida joylashgan hujjatlarni yaxshiroq tushunishi mumkin. Biroq, modomiki hujjat kontekst ichiga kiritilgan ekan, uning tartibi qidiruv tizimlaridagi darajalashga nisbatan kamroq ahamiyat kasb etadi.

### So'rovni qayta yozish

So'rovni qayta yozish (_query rewriting_) — **so'rovni qayta shakllantirish** (_query reformulation_), **so'rovni normallashtirish** (_query normalization_) va ba'zan **so'rovni kengaytirish** (_query expansion_) deb ham ataladi. Quyidagi suhbatni ko'rib chiqaylik:

> **Foydalanuvchi:** John Doe bizdan oxirgi marta qachon biror narsa sotib olgan edi?
>
> **_SI_:** John bizdan oxirgi marta ikki hafta oldin, 2030-yil 3-yanvarda "Fruity Fedora" shlyapasini sotib olgan.
>
> **Foydalanuvchi:** Emily Doe-chi?

Oxirgi savol, "Emily Doe-chi?", kontekstsiz mavhumdir. Agar siz ushbu so'rovdan hujjatlarni qidirishda so'zma-so'z foydalansangiz, katta ehtimol bilan aloqasi bo'lmagan natijalarni olasiz. Siz ushbu so'rovni foydalanuvchi aslida nimani so'rayotganini aks ettiradigan qilib qayta yozishingiz kerak. Yangi so'rov o'z-o'zidan ma'noga ega bo'lishi lozim. Ushbu holatda, so'rov "Emily Doe bizdan oxirgi marta qachon biror narsa sotib olgan edi?" deb qayta yozilishi kerak.

Men so'rovni qayta yozishni 253-betdagi "_RAG_" bo'limiga kiritgan bo'lsam-da, bu usul faqat _RAG_'ga xos emas. An'anaviy qidiruv tizimlarida so'rovni qayta yozish ko'pincha evristikalar yordamida amalga oshiriladi. _SI_ ilovalarida so'rovni qayta yozish boshqa _SI_ modellari yordamida, "Quyidagi suhbatni inobatga olgan holda, foydalanuvchining oxirgi kiruvchi ma'lumotini u aslida nimani so'rayotganini aks ettiradigan qilib qayta yozing" kabi prompt orqali ham bajarilishi mumkin. 6-4-rasmda ChatGPT ushbu prompt yordamida so'rovni qanday qayta yozgani ko'rsatilgan.

![6-4-rasm. So'rovlarni qayta yozish uchun boshqa generativ modellardan foydalanishingiz mumkin.](/ai-engineering/6-chapter/6.4-figure.png)
<div className='text-center text-sm italic'>6-4-rasm. So'rovlarni qayta yozish uchun boshqa generativ modellardan foydalanishingiz mumkin.</div>

So'rovni qayta yozish, ayniqsa, **shaxsni aniqlashtirish** (_identity resolution_) yoki boshqa bilimlarni jalb qilish kerak bo'lganda murakkablashishi mumkin. Masalan, agar foydalanuvchi "Uning xotini-chi?" deb so'rasa, avval uning xotini kimligini bilish uchun ma'lumotlar bazangizga murojaat qilishingiz kerak bo'ladi. Agar sizda bu ma'lumot bo'lmasa, qayta yozuvchi model noto'g'ri javobga yetaklovchi biror ismni gallyutsinatsiya qilish o'rniga, ushbu so'rovni yechib bo'lmasligini tan olishi kerak.