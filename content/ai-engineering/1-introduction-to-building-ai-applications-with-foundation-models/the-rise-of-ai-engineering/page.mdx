# Sun'iy intellekt muhandisligining yuksalishi

Fundamental modellar katta til modellaridan, ular esa, o'z navbatida, oddiy til modellaridan kelib chiqqan. `ChatGPT` va `GitHub Copilot` kabi dasturlar go'yo hech qayerdan paydo bo'lib qolgandek tuyulsa-da, aslida ular o'nlab yillik texnologik taraqqiyotning mahsulidir, zero birinchi til modellari 1950-yillarda paydo bo'lgan. Ushbu bo'limda til modellaridan sun'iy intellekt muhandisligigacha bo'lgan evolyutsiyaga imkon bergan eng muhim yutuqlar izma-iz ko'rib chiqiladi.

## Til modellari (_LM_) dan katta til modellari (_LLM_) ga

Til modellari ancha vaqtdan beri mavjud bo'lsa-da, ular faqatgina o'z-o'zini nazorat qilish (`self-supervision`) yordamidagina bugungi miqyosga erisha oldi. Ushbu qismda til modeli va o'z-o'zini nazorat qilish tushunchalari qisqacha sharhlanadi. Agar bu tushunchalar bilan allaqachon tanish bo'lsangiz, bu qismni o'tkazib yuborishingiz mumkin.

## Til modellari

Til modeli bir yoki bir nechta til haqidagi statistik ma'lumotlarni o'zida jamlaydi. Mohiyatan, bu ma'lumotlar biror so'zning ma'lum bir kontekstda paydo bo'lish ehtimoli qanchalik ekanligini bildiradi. Masalan, “Mening sevimli rangim __” degan kontekst berilganda, ingliz tilini o'zida jamlagan til modeli “mashina” so'ziga qaraganda “ko'k” so'zini ko'proq bashorat qilishi kerak.

Tillarning statistik tabiati asrlar oldin kashf etilgan. 1905-yilda yozilgan [“Raqsga tushayotgan odamchalar sarguzashti”](https://en.wikipedia.org/wiki/The_Adventure_of_the_Dancing_Men) hikoyasida Sherlok Holms sirli tayoqcha shakllar ketma-ketligining shifrini yechish uchun ingliz tilining oddiy statistik ma'lumotlaridan foydalangan. Ingliz tilida eng ko'p uchraydigan harf _E_ bo'lgani uchun, Holms eng ko'p uchraydigan tayoqcha shakli _E_ harfini anglatishi kerak degan xulosaga kelgan.

Keyinroq, Klod Shennon Ikkinchi jahon urushi paytida dushman xabarlarining shifrini ochish uchun yanada murakkabroq statistikadan foydalangan. Uning ingliz tilini qanday modellashtirish haqidagi ishi 1951-yilda chop etilgan va o'z davrida tub burilish yasagan [“Bosma ingliz tilining bashorati va entropiyasi”](https://www.kuenzigbooks.com/pages/books/28623/c-e-shannon-claude-elwood/prediction-and-entropy-of-printed-english-bell-monograph) (`Prediction and Entropy of Printed English`) nomli maqolasida nashr etilgan. Ushbu maqolada kiritilgan ko'plab tushunchalar, jumladan, entropiya, bugungi kunda ham til modellashtirishda qo'llaniladi.

Dastlabki paytlarda til modeli faqat bitta tilni o'z ichiga olardi. Biroq, bugungi kunda til modeli bir nechta tilni qamrab olishi mumkin.

Til modelining asosiy birligi — bu token. Token — bu modelga qarab, belgi, so'z yoki so'zning bir qismi (masalan, `-tion` kabi) bo'lishi mumkin.[^1] Masalan, `ChatGPT` ortida turgan model — `GPT-4` — “I can't wait to build AI applications” iborasini 1-1-rasmda ko'rsatilganidek, to'qqizta tokenga ajratadi. E'tibor bering, bu misolda “can't” so'zi ikkita tokenga — `can` va `'t`'ga ajratilgan. Turli `OpenAI` modellarining matnni qanday tokenlarga ajratishini `OpenAI` veb-saytida ko'rishingiz mumkin.

![GPT-4 modeli "I can't wait to build AI applications" iborasini tokenlarga ajratish](/ai-engineering/1.1-figure.png)
<div className='text-center text-sm'>_1-1 rasm. GPT-4 modeli "I can't wait to build AI applications" iborasini tokenlarga ajratish_</div>

Asl matnni tokenlarga ajratish jarayoni **tokenizatsiya** deb ataladi. `GPT-4` uchun o'rtacha bitta token [so'z uzunligining taxminan ¾ qismiga](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) teng. Demak, 100 ta token taxminan 75 ta so'zga to'g'ri keladi.

Model ishlay oladigan barcha tokenlar to'plami **modelning lug'ati** (`vocabulary`) deb ataladi. Xuddi alifbodagi sanoqli harflardan foydalanib ko'plab so'zlarni hosil qilish mumkin bo'lganidek, cheklangan miqdordagi tokenlar yordamida ham juda ko'p sonli turli xil so'zlarni yaratish mumkin. [`Mixtral 8x7B`](https://mistral.ai/news/mixtral-of-experts) modelining lug'at hajmi 32 000 tani tashkil etadi. `GPT-4`'ning lug'at hajmi esa [100 256](https://github.com/openai/tiktoken/blob/main/tiktoken/model.py) ga teng. Tokenizatsiya usuli va lug'at hajmi model yaratuvchilari tomonidan belgilanadi.

<Callout>
### Nima uchun til modellari so'z yoki belgilar o'rniga tokenlardan foydalanadi?

Til modellari o'zining asosiy birligi sifatida so'z yoki belgilarni emas, aynan tokenlarni ishlatishining uchta asosiy sababi bor:

1. **Ma'noli qismlarga ajratish.** Belgilarga nisbatan, tokenlar modelga so'zlarni ma'noli tarkibiy qismlarga ajratish imkonini beradi. Masalan, “cooking” (pishirish) so'zini “cook” (pishirmoq) va “ing” (jarayonni bildiruvchi qo'shimcha) qismlariga ajratish mumkin, bunda ikkala qism ham asl so'zning ma'nosidan bir qismini o'zida saqlab qoladi.

2. **Samaradorlik.** Noyob tokenlar soni noyob so'zlar sonidan kamroq bo'lgani uchun, bu modelning lug'at hajmini qisqartiradi va uni yanada samaraliroq qiladi (bu haqda 2-bobda muhokama qilinadi).

3. **Noma'lum so'zlarni qayta ishlash.** Tokenlar, shuningdek, modelga noma'lum so'zlarni qayta ishlashga yordam beradi. Masalan, “chatgpting” kabi o'ylab topilgan so'zni “chatgpt” va “ing” qismlariga ajratish mumkin, bu esa modelga uning tuzilishini tushunishga yordam beradi. Shunday qilib, tokenlar bir tomondan birliklar sonini so'zlarga qaraganda kamroq ushlab tursa, ikkinchi tomondan, alohida belgilarga qaraganda ko'proq ma'no saqlab qolish o'rtasidagi muvozanatni ta'minlaydi.
</Callout>

Til modellarining ikkita asosiy turi mavjud: _niqoblangan (masked) til modellari_ va _avtoregressiv (autoregressive) til modellari_. Bu ikki tur bir-biridan tokenni bashorat qilishda qanday ma'lumotlardan foydalanishi bilan ajralib turadi:

### Niqoblangan til modellari

Niqoblangan til modeli ketma-ketlikning istalgan joyidagi tushib qolgan tokenlarni, ham oldingi, ham keyingi kontekstdan foydalangan holda, bashorat qilishga o'rgatiladi. Mohiyatan, niqoblangan til modeli bo'sh joyni to'ldirishga o'rgatiladi. Masalan, “Mening sevimli __ ko'k” degan kontekst berilsa, niqoblangan til modeli bo'sh joy “rang” so'zi bo'lishi ehtimoli yuqori ekanligini bashorat qilishi kerak. Niqoblangan til modelining mashhur namunasi — bu transformatorlardan olingan ikki tomonlama kodlovchi tasvirlar (`bidirectional encoder representations from transformers`), ya'ni `BERT` ([Devlin va boshq., 2018](https://arxiv.org/abs/1810.04805)).

Ushbu kitob yozilayotgan vaqtda, niqoblangan til modellari odatda hissiyot tahlili (`sentiment analysis`) va matn tasnifi (`text classification`) kabi generativ bo'lmagan vazifalar uchun qo'llaniladi. Ular, shuningdek, koddagi xatolarni tuzatish (`code debugging`) kabi umumiy kontekstni tushunishni talab qiladigan vazifalar uchun ham foydalidir, chunki bunda model xatolarni aniqlash uchun ham oldingi, ham keyingi kodni tushunishi kerak bo'ladi.

### Avtoregressiv til modellari

Avtoregressiv til modeli ketma-ketlikdagi keyingi tokenni faqat o'zidan oldingi tokenlardan foydalangan holda bashorat qilishga o'rgatiladi. U “Mening sevimli rangim __” jumlasida keyin nima kelishini bashorat qiladi.[^2] Avtoregressiv model uzluksiz ravishda birin-ketin token generatsiya qila oladi. Bugungi kunda avtoregressiv til modellari matn generatsiyasi uchun eng maqbul modellar hisoblanadi va shu sababli ular niqoblangan til modellariga qaraganda ancha ommaboproqdir.[^3]

1-2-rasmda til modellarining bu ikki turi ko'rsatilgan.

![1-2-rasm. Avtoregressiv til modeli va niqoblangan til modeli](/ai-engineering/1.2-figure.png)

<div className='text-center text-sm'>_1-2 rasm. Avtoregressiv til modeli va niqoblangan til modeli_</div>

<Callout>
### Eslatma

Ushbu kitobda, agar alohida ta'kidlanmagan bo'lsa, “til modeli” deganda avtoregressiv model nazarda tutiladi.
</Callout>

Til modellarining natijalari erkin va cheklanmagandir. Til modeli o'zining qat'iy, cheklangan lug'atidan foydalanib, cheksiz miqdordagi ehtimoliy natijalarni yarata oladi. Erkin natijalar generatsiya qila oladigan model **generativ** deb ataladi, shu sababli **generativ SI** atamasi kelib chiqqan.

Til modelini bir **to'ldiruvchi mashina** deb tasavvur qilish mumkin: unga biror matn (`prompt`) berilsa, u shu matnni to'ldirishga harakat qiladi. Mana bir misol:

``` markdown
Prompt (foydalanuvchidan): "Bo'lmoq yoki bo'lmaslik"
Davomi (til modelidan): ", mana masalaning asl mohiyati."
```

Shuni ta'kidlash muhimki, bu to'ldirmalar ehtimolliklarga asoslangan bashoratlar bo'lib, ularning to'g'ri bo'lishi kafolatlanmagan. Til modellarining aynan shu ehtimoliy tabiati ulardan foydalanishni bir vaqtning o'zida ham hayajonli, ham asabiylashtiruvchi qiladi. Bu mavzuni 2-bobda yanada chuqurroq o'rganamiz.

Garchi sodda eshitilsa-da, to'ldirish jarayoni nihoyatda qudratli imkoniyatdir. Tarjima, qisqacha bayon qilish, kod yozish va matematik masalalarni yechish kabi ko'plab vazifalarni to'ldirish vazifalari sifatida ifodalash mumkin. Masalan, “Fransuz tilida 'ahvollaringiz qalay' … degani” `prompt`'i berilganda, til modeli uni “Comment ça va” bilan to'ldirishi mumkin, bu esa bir tildan ikkinchisiga samarali tarjima qilish demakdir.

Yana bir misol, quyidagi `prompt` berilganda:

``` markdown
Savol: Ushbu elektron pochta spam bo'lishi ehtimoli bormi? Xatning matni: <elektron pochta matni>
Javob:
```

Til modeli uni “Ehtimoliy spam” deb to'ldirishi mumkin, bu esa o'z navbatida til modelini spam-klassifikatorga aylantiradi.

To'ldirish qudratli bo'lsa-da, bu suhbat qurish bilan bir xil narsa emas. Masalan, agar siz to'ldiruvchi mashinaga savol bersangiz, u savolga javob berish o'rniga, sizning gapingizni yana bir savol qo'shish orqali to'ldirishi mumkin. “O'qitishdan keyingi bosqich” (`Post-Training`) bo'limida modelni foydalanuvchi so'roviga qanday qilib munosib javob berishga o'rgatish muhokama qilinadi.

## O'z-o'zini nazorat qilish (Self-supervision)

Til modellashtirish — bu ko'plab _ML_ algoritmlaridan faqat bittasigina xolos. Bundan tashqari, obyektlarni aniqlash, mavzularni modellashtirish, tavsiya tizimlari, ob-havoni bashorat qilish, aksiya narxlarini taxmin qilish va hokazolar uchun ham modellar mavjud. Xo'sh, til modellarining boshqa ko'plab _ML_ algoritmlaridan qanday afzalligi bor va nima ularni `ChatGPT` inqilobiga olib kelgan miqyoslash yondashuvining markaziga aylantirdi?

Javob shundaki, til modellarini o'z-o'zini nazorat qilish (`self-supervision`) yordamida o'qitish mumkin, holbuki, boshqa ko'plab modellar nazoratli o'qitishni (`supervision`) talab qiladi. Nazoratli o'qitish — bu _ML_ algoritmlarini maxsus belgilangan ma'lumotlar yordamida o'qitish jarayonidir va bunday ma'lumotlarni to'plash qimmatga tushishi va ko'p vaqt talab qilishi mumkin. O'z-o'zini nazorat qilish esa aynan shu ma'lumotlarni belgilashdagi to'siqni yengib o'tishga yordam beradi. Bu esa, o'z navbatida, modellarga o'rganish uchun kattaroq ma'lumotlar to'plamlarini yaratish va ularning miqyosini samarali ravishda kengaytirish imkonini beradi. Keling, buni batafsil ko'rib chiqamiz.

Nazoratli o'qitishda siz model o'rganishi kerak bo'lgan xususiyatlarni ko'rsatish uchun namunalarni belgilab chiqasiz va keyin modelni shu namunalar asosida o'qitasiz. O'qitib bo'lingach, modelni yangi ma'lumotlarga qo'llash mumkin bo'ladi. Masalan, firibgarlikni aniqlash modelini o'qitish uchun siz har biri “firibgarlik” yoki “firibgarlik emas” deb belgilangan tranzaksiyalar namunalaridan foydalanasiz. Model bu namunalardan o'rganib olgach, siz undan yangi tranzaksiyaning firibgarlik ekanligini bashorat qilish uchun foydalanishingiz mumkin.

2010-yillarda SI modellarining muvaffaqiyati aynan nazoratli o'qitishga asoslangan edi. Chuqur o'rganish (`deep learning`) inqilobini boshlab bergan model — _AlexNet_ ([Krizhevsky va boshq., 2012](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)) — nazoratli o'qitishga asoslangan edi. U _ImageNet_ ma'lumotlar to'plamidagi 1 milliondan ortiq rasmni tasniflashni o'rganish uchun o'qitilgan. U har bir rasmni “mashina”, “havo shari” yoki “maymun” kabi 1000 ta toifadan biriga ajratgan.

Nazoratli o'qitishning kamchiligi shundaki, ma'lumotlarni belgilash qimmat va ko'p vaqt talab qiladigan jarayondir. Agar bir kishi bitta rasmni belgilashi uchun 5 sent ketsa, _ImageNet_ uchun millionta rasmni belgilash 50 000 dollarga tushadi.[^4] Agar siz har bir rasmni ikki xil odam belgilashini istasangiz — bu orqali belgilash sifatini qayta tekshirish mumkin bo'ladi — xarajat ikki baravar oshadi. Dunyoda 1000 tadan ancha ko'p obyekt mavjudligini hisobga olsak, modellarning imkoniyatlarini ko'proq obyektlar bilan ishlashga kengaytirish uchun siz ko'proq toifalarga oid belgilarni qo'shishingiz kerak bo'ladi. Miqyosni 1 million toifaga yetkazish uchun esa, faqat belgilash xarajatining o'zi 50 million dollarga yetadi.

Kundalik obyektlarni belgilashni ko'pchilik odamlar maxsus tayyorgarliksiz ham bajara oladi. Shu sababli, buni nisbatan arzon amalga oshirish mumkin. Biroq, hamma belgilash vazifalari ham bunchalik oson emas. Ingliz tilidan lotin tiliga tarjima qiluvchi model uchun lotincha tarjimalarni yaratish ancha qimmatroq. Kompyuter tomografiyasi (`CT scan`) suratida saraton belgilari bor-yo'qligini belgilash esa tasavvur qilib bo'lmas darajada qimmatga tushardi.

O'z-o'zini nazorat qilish ma'lumotlarni belgilashdagi bu to'siqni yengib o'tishga yordam beradi. O'z-o'zini nazorat qilishda model aniq belgilarni talab qilish o'rniga, belgilarni kiruvchi ma'lumotlarning o'zidan chiqarib oladi. Til modellashtirish o'z-o'zini nazorat qilishga asoslanadi, chunki har bir kiruvchi ketma-ketlik ham belgilarni (bashorat qilinishi kerak bo'lgan tokenlarni), ham model bu belgilarni bashorat qilish uchun foydalanishi mumkin bo'lgan kontekstni o'zida mujassam etadi. Masalan, "Men ko'cha taomlarini yaxshi ko'raman." ("I love street food.") jumlasi 1-1-jadvalda ko'rsatilganidek, oltita o'qitish namunasini beradi.

| Kirish (kontekst) | Chiqish (keyingi token) |
| :--- | :--- |
| `<BOS>` | I |
| `<BOS>`, I | love |
| `<BOS>`, I, love | street |
| `<BOS>`, I, love, street | food |
| `<BOS>`, I, love, street, food | . |
| `<BOS>`, I, love, street, food, . | `<EOS>` |

<div className='text-center text-sm italic'>1-1 jadval. "I love street food." jumlasidan til modellashtirish uchun olingan o'qitish namunalari.</div>

1-1-jadvalda `<BOS>` va `<EOS>` ketma-ketlikning boshlanishi va tugashini bildiradi. Bu belgilar til modelining bir nechta ketma-ketliklar bilan ishlashi uchun zarur. Har bir belgi odatda model tomonidan bitta maxsus token sifatida qabul qilinadi. Ketma-ketlikning tugash belgisi ayniqsa muhim, chunki u til modellariga o'z javoblarini qachon tugatish kerakligini bilishga yordam beradi.[^5]

<Callout>
### Eslatma

O'z-o'zini nazorat qilish (`self-supervision`) nazoratsiz o'qitishdan (`unsupervision`) farq qiladi. O'z-o'zini nazorat qilishda belgilar kiruvchi ma'lumotlarning o'zidan chiqarib olinsa, nazoratsiz o'qitishda esa belgilarga umuman hojat yo'q.
</Callout>

O'z-o'zini nazorat qilish til modellarining hech qanday belgilashni talab qilmasdan, to'g'ridan-to'g'ri matn ketma-ketliklaridan o'rganishini anglatadi. Matn ketma-ketliklari hamma joyda — kitoblarda, blog postlarida, maqolalarda va `Reddit` izohlarida — mavjud bo'lgani uchun, ulkan hajmdagi o'qitish ma'lumotlarini to'plash mumkin. Bu esa til modellarining miqyosini kengaytirib, katta til modellariga (_LLM_) aylanishiga imkon beradi.

Biroq, _LLM_ atamasi ilmiy jihatdan unchalik aniq emas. Til modeli “katta” deb hisoblanishi uchun qanchalik katta bo'lishi kerak? Bugun katta deb hisoblangan narsa ertaga kichik bo'lib qolishi mumkin. Modelning hajmi odatda uning parametrlari soni bilan o'lchanadi. Parametr — bu _ML_ modelidagi o'zgaruvchi bo'lib, u o'qitish jarayonida yangilanib boradi.[^6] Umuman olganda, garchi bu har doim ham to'g'ri bo'lmasa-da, modelda qancha ko'p parametr bo'lsa, uning kerakli xususiyatlarni o'rganish salohiyati shuncha yuqori bo'ladi.

2018-yil iyun oyida `OpenAI`'ning birinchi generativ oldindan o'qitilgan transformeri (`GPT`) chiqqanida, uning 117 million parametri bor edi va bu katta hisoblanardi. 2019-yil fevral oyida `OpenAI` 1.5 milliard parametrli `GPT-2`'ni taqdim etganida, 117 million endi kichik deb hisoblana boshlandi. Ushbu kitob yozilayotgan vaqtda, 100 milliard parametrli model katta hisoblanadi. Balki bir kun kelib bu hajm ham kichik bo'lib qolar.

Keyingi bo'limga o'tishdan oldin, odatda e'tibordan chetda qoladigan bir savolga to'xtalib o'tmoqchiman: _Nima uchun kattaroq modellarga ko'proq ma'lumot kerak?_ Kattaroq modellar o'rganish uchun ko'proq salohiyatga ega, shuning uchun ularning samaradorligini maksimal darajaga chiqarish uchun ko'proq o'qitish ma'lumotlari kerak bo'ladi.[^7] Katta modelni kichik ma'lumotlar to'plamida ham o'qitish mumkin, lekin bu hisoblash resurslarini behuda sarflash bo'ladi. Bu ma'lumotlar to'plamida kichikroq modellar yordamida ham xuddi shunday yoki undan ham yaxshiroq natijalarga erishish mumkin edi.

[^1]: Ingliz tilidan boshqa tillarda bitta `Unicode` belgisi ba'zan bir nechta token sifatida ifodalanishi mumkin.

[^2]: Avtoregressiv til modellari ba'zan sababiy til modellari ([`causal language models`](https://huggingface.co/docs/transformers/en/tasks/language_modeling)) deb ham ataladi.

[^3]: Texnik jihatdan, agar juda qattiq harakat qilinsa, `BERT` kabi niqoblangan til modelidan ham matn generatsiyasi uchun foydalanish mumkin.

[^4]: Ma'lumotlarni belgilashning haqiqiy narxi bir nechta omillarga, jumladan, vazifaning murakkabligi, miqyosi (kattaroq ma'lumotlar to'plamlari odatda har bir namuna uchun arzonroq narxga olib keladi) va belgilash xizmatini taqdim etuvchi provayderga bog'liq. Masalan, 2024-yil sentabr holatiga ko'ra, [`Amazon SageMaker Ground Truth`](https://aws.amazon.com/sagemaker/ai/groundtruth/) 50 000 tadan kam rasmni belgilash uchun har bir rasmga 8 sent, 1 milliondan ortiq rasmni belgilash uchun esa har bir rasmga atigi 2 sent oladi.

[^5]: Bu xuddi insonlar uchun qachon gapirishni to'xtatishni bilish muhim bo'lganiga o'xshaydi.

[^6]: Maktabda menga model parametrlari ham model og'irliklari (`weights`), ham model siljishlari (`biases`)ni o'z ichiga oladi deb o'rgatishgan. Biroq, bugungi kunda biz odatda barcha parametrlarni nazarda tutib, model og'irliklari atamasini ishlatamiz.

[^7]: Kattaroq modellarga ko'proq o'qitish ma'lumotlari kerakligi mantiqqa zid tuyulishi mumkin. Agar model qudratliroq bo'lsa, o'rganish uchun kamroq namuna talab qilishi kerak emasmi? Biroq, bizning maqsadimiz katta modelning bir xil ma'lumotlar yordamida kichik modelning samaradorligiga erishishi emas. Bizning maqsadimiz — model samaradorligini maksimal darajaga chiqarishdir.