# Sun'iy intellekt muhandisligining yuksalishi

Fundamental modellar katta til modellaridan, ular esa, o'z navbatida, oddiy til modellaridan kelib chiqqan. `ChatGPT` va `GitHub Copilot` kabi dasturlar go'yo hech qayerdan paydo bo'lib qolgandek tuyulsa-da, aslida ular o'nlab yillik texnologik taraqqiyotning mahsulidir, zero birinchi til modellari 1950-yillarda paydo bo'lgan. Ushbu bo'limda til modellaridan sun'iy intellekt muhandisligigacha bo'lgan evolyutsiyaga imkon bergan eng muhim yutuqlar izma-iz ko'rib chiqiladi.

## Til modellari (_LM_) dan katta til modellari (_LLM_) ga

Til modellari ancha vaqtdan beri mavjud bo'lsa-da, ular faqatgina o'z-o'zini nazorat qilish (`self-supervision`) yordamidagina bugungi miqyosga erisha oldi. Ushbu qismda til modeli va o'z-o'zini nazorat qilish tushunchalari qisqacha sharhlanadi. Agar bu tushunchalar bilan allaqachon tanish bo'lsangiz, bu qismni o'tkazib yuborishingiz mumkin.

### Til modellari

Til modeli bir yoki bir nechta til haqidagi statistik ma'lumotlarni o'zida jamlaydi. Mohiyatan, bu ma'lumotlar biror so'zning ma'lum bir kontekstda paydo bo'lish ehtimoli qanchalik ekanligini bildiradi. Masalan, “Mening sevimli rangim __” degan kontekst berilganda, ingliz tilini o'zida jamlagan til modeli “mashina” so'ziga qaraganda “ko'k” so'zini ko'proq bashorat qilishi kerak.

Tillarning statistik tabiati asrlar oldin kashf etilgan. 1905-yilda yozilgan [“Raqsga tushayotgan odamchalar sarguzashti”](https://en.wikipedia.org/wiki/The_Adventure_of_the_Dancing_Men) hikoyasida Sherlok Holms sirli tayoqcha shakllar ketma-ketligining shifrini yechish uchun ingliz tilining oddiy statistik ma'lumotlaridan foydalangan. Ingliz tilida eng ko'p uchraydigan harf _E_ bo'lgani uchun, Holms eng ko'p uchraydigan tayoqcha shakli _E_ harfini anglatishi kerak degan xulosaga kelgan.

Keyinroq, Klod Shennon Ikkinchi jahon urushi paytida dushman xabarlarining shifrini ochish uchun yanada murakkabroq statistikadan foydalangan. Uning ingliz tilini qanday modellashtirish haqidagi ishi 1951-yilda chop etilgan va o'z davrida tub burilish yasagan [“Bosma ingliz tilining bashorati va entropiyasi”](https://www.kuenzigbooks.com/pages/books/28623/c-e-shannon-claude-elwood/prediction-and-entropy-of-printed-english-bell-monograph) (`Prediction and Entropy of Printed English`) nomli maqolasida nashr etilgan. Ushbu maqolada kiritilgan ko'plab tushunchalar, jumladan, entropiya, bugungi kunda ham til modellashtirishda qo'llaniladi.

Dastlabki paytlarda til modeli faqat bitta tilni o'z ichiga olardi. Biroq, bugungi kunda til modeli bir nechta tilni qamrab olishi mumkin.

Til modelining asosiy birligi — bu token. Token — bu modelga qarab, belgi, so'z yoki so'zning bir qismi (masalan, `-tion` kabi) bo'lishi mumkin.[^1] Masalan, `ChatGPT` ortida turgan model — `GPT-4` — “I can't wait to build AI applications” iborasini 1-1-rasmda ko'rsatilganidek, to'qqizta tokenga ajratadi. E'tibor bering, bu misolda “can't” so'zi ikkita tokenga — `can` va `'t`'ga ajratilgan. Turli `OpenAI` modellarining matnni qanday tokenlarga ajratishini `OpenAI` veb-saytida ko'rishingiz mumkin.

![GPT-4 modeli "I can't wait to build AI applications" iborasini tokenlarga ajratish](/ai-engineering/1.1-figure.png)
<div className='text-center text-sm'>_1-1 rasm. GPT-4 modeli "I can't wait to build AI applications" iborasini tokenlarga ajratish_</div>

Asl matnni tokenlarga ajratish jarayoni **tokenizatsiya** deb ataladi. `GPT-4` uchun o'rtacha bitta token [so'z uzunligining taxminan ¾ qismiga](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) teng. Demak, 100 ta token taxminan 75 ta so'zga to'g'ri keladi.

Model ishlay oladigan barcha tokenlar to'plami **modelning lug'ati** (`vocabulary`) deb ataladi. Xuddi alifbodagi sanoqli harflardan foydalanib ko'plab so'zlarni hosil qilish mumkin bo'lganidek, cheklangan miqdordagi tokenlar yordamida ham juda ko'p sonli turli xil so'zlarni yaratish mumkin. [`Mixtral 8x7B`](https://mistral.ai/news/mixtral-of-experts) modelining lug'at hajmi 32 000 tani tashkil etadi. `GPT-4`'ning lug'at hajmi esa [100 256](https://github.com/openai/tiktoken/blob/main/tiktoken/model.py) ga teng. Tokenizatsiya usuli va lug'at hajmi model yaratuvchilari tomonidan belgilanadi.

<Callout>
### Nima uchun til modellari so'z yoki belgilar o'rniga tokenlardan foydalanadi?

Til modellari o'zining asosiy birligi sifatida so'z yoki belgilarni emas, aynan tokenlarni ishlatishining uchta asosiy sababi bor:

1. **Ma'noli qismlarga ajratish.** Belgilarga nisbatan, tokenlar modelga so'zlarni ma'noli tarkibiy qismlarga ajratish imkonini beradi. Masalan, “cooking” (pishirish) so'zini “cook” (pishirmoq) va “ing” (jarayonni bildiruvchi qo'shimcha) qismlariga ajratish mumkin, bunda ikkala qism ham asl so'zning ma'nosidan bir qismini o'zida saqlab qoladi.

2. **Samaradorlik.** Noyob tokenlar soni noyob so'zlar sonidan kamroq bo'lgani uchun, bu modelning lug'at hajmini qisqartiradi va uni yanada samaraliroq qiladi (bu haqda 2-bobda muhokama qilinadi).

3. **Noma'lum so'zlarni qayta ishlash.** Tokenlar, shuningdek, modelga noma'lum so'zlarni qayta ishlashga yordam beradi. Masalan, “chatgpting” kabi o'ylab topilgan so'zni “chatgpt” va “ing” qismlariga ajratish mumkin, bu esa modelga uning tuzilishini tushunishga yordam beradi. Shunday qilib, tokenlar bir tomondan birliklar sonini so'zlarga qaraganda kamroq ushlab tursa, ikkinchi tomondan, alohida belgilarga qaraganda ko'proq ma'no saqlab qolish o'rtasidagi muvozanatni ta'minlaydi.
</Callout>

Til modellarining ikkita asosiy turi mavjud: _niqoblangan (masked) til modellari_ va _avtoregressiv (autoregressive) til modellari_. Bu ikki tur bir-biridan tokenni bashorat qilishda qanday ma'lumotlardan foydalanishi bilan ajralib turadi:

### Niqoblangan til modellari

Niqoblangan til modeli ketma-ketlikning istalgan joyidagi tushib qolgan tokenlarni, ham oldingi, ham keyingi kontekstdan foydalangan holda, bashorat qilishga o'rgatiladi. Mohiyatan, niqoblangan til modeli bo'sh joyni to'ldirishga o'rgatiladi. Masalan, “Mening sevimli __ ko'k” degan kontekst berilsa, niqoblangan til modeli bo'sh joy “rang” so'zi bo'lishi ehtimoli yuqori ekanligini bashorat qilishi kerak. Niqoblangan til modelining mashhur namunasi — bu transformatorlardan olingan ikki tomonlama kodlovchi tasvirlar (`bidirectional encoder representations from transformers`), ya'ni `BERT` ([Devlin va boshq., 2018](https://arxiv.org/abs/1810.04805)).

Ushbu kitob yozilayotgan vaqtda, niqoblangan til modellari odatda hissiyot tahlili (`sentiment analysis`) va matn tasnifi (`text classification`) kabi generativ bo'lmagan vazifalar uchun qo'llaniladi. Ular, shuningdek, koddagi xatolarni tuzatish (`code debugging`) kabi umumiy kontekstni tushunishni talab qiladigan vazifalar uchun ham foydalidir, chunki bunda model xatolarni aniqlash uchun ham oldingi, ham keyingi kodni tushunishi kerak bo'ladi.

### Avtoregressiv til modellari

Avtoregressiv til modeli ketma-ketlikdagi keyingi tokenni faqat o'zidan oldingi tokenlardan foydalangan holda bashorat qilishga o'rgatiladi. U “Mening sevimli rangim __” jumlasida keyin nima kelishini bashorat qiladi.[^2] Avtoregressiv model uzluksiz ravishda birin-ketin token generatsiya qila oladi. Bugungi kunda avtoregressiv til modellari matn generatsiyasi uchun eng maqbul modellar hisoblanadi va shu sababli ular niqoblangan til modellariga qaraganda ancha ommaboproqdir.[^3]

1-2-rasmda til modellarining bu ikki turi ko'rsatilgan.

![1-2-rasm. Avtoregressiv til modeli va niqoblangan til modeli](/ai-engineering/1.2-figure.png)

<div className='text-center text-sm'>_1-2 rasm. Avtoregressiv til modeli va niqoblangan til modeli_</div>

<Callout>
### Eslatma

Ushbu kitobda, agar alohida ta'kidlanmagan bo'lsa, “til modeli” deganda avtoregressiv model nazarda tutiladi.
</Callout>

Til modellarining natijalari erkin va cheklanmagandir. Til modeli o'zining qat'iy, cheklangan lug'atidan foydalanib, cheksiz miqdordagi ehtimoliy natijalarni yarata oladi. Erkin natijalar generatsiya qila oladigan model **generativ** deb ataladi, shu sababli **generativ SI** atamasi kelib chiqqan.

Til modelini bir **to'ldiruvchi mashina** deb tasavvur qilish mumkin: unga biror matn (`prompt`) berilsa, u shu matnni to'ldirishga harakat qiladi. Mana bir misol:

``` markdown
Prompt (foydalanuvchidan): "Bo'lmoq yoki bo'lmaslik"
Davomi (til modelidan): ", mana masalaning asl mohiyati."
```

Shuni ta'kidlash muhimki, bu to'ldirmalar ehtimolliklarga asoslangan bashoratlar bo'lib, ularning to'g'ri bo'lishi kafolatlanmagan. Til modellarining aynan shu ehtimoliy tabiati ulardan foydalanishni bir vaqtning o'zida ham hayajonli, ham asabiylashtiruvchi qiladi. Bu mavzuni 2-bobda yanada chuqurroq o'rganamiz.

Garchi sodda eshitilsa-da, to'ldirish jarayoni nihoyatda qudratli imkoniyatdir. Tarjima, qisqacha bayon qilish, kod yozish va matematik masalalarni yechish kabi ko'plab vazifalarni to'ldirish vazifalari sifatida ifodalash mumkin. Masalan, “Fransuz tilida 'ahvollaringiz qalay' … degani” `prompt`'i berilganda, til modeli uni “Comment ça va” bilan to'ldirishi mumkin, bu esa bir tildan ikkinchisiga samarali tarjima qilish demakdir.

Yana bir misol, quyidagi `prompt` berilganda:

``` markdown
Savol: Ushbu elektron pochta spam bo'lishi ehtimoli bormi? Xatning matni: <elektron pochta matni>
Javob:
```

Til modeli uni “Ehtimoliy spam” deb to'ldirishi mumkin, bu esa o'z navbatida til modelini spam-klassifikatorga aylantiradi.

To'ldirish qudratli bo'lsa-da, bu suhbat qurish bilan bir xil narsa emas. Masalan, agar siz to'ldiruvchi mashinaga savol bersangiz, u savolga javob berish o'rniga, sizning gapingizni yana bir savol qo'shish orqali to'ldirishi mumkin. “O'qitishdan keyingi bosqich” (`Post-Training`) bo'limida modelni foydalanuvchi so'roviga qanday qilib munosib javob berishga o'rgatish muhokama qilinadi.

### O'z-o'zini nazorat qilish (Self-supervision)

Til modellashtirish — bu ko'plab _ML_ algoritmlaridan faqat bittasigina xolos. Bundan tashqari, obyektlarni aniqlash, mavzularni modellashtirish, tavsiya tizimlari, ob-havoni bashorat qilish, aksiya narxlarini taxmin qilish va hokazolar uchun ham modellar mavjud. Xo'sh, til modellarining boshqa ko'plab _ML_ algoritmlaridan qanday afzalligi bor va nima ularni `ChatGPT` inqilobiga olib kelgan miqyoslash yondashuvining markaziga aylantirdi?

Javob shundaki, til modellarini o'z-o'zini nazorat qilish (`self-supervision`) yordamida o'qitish mumkin, holbuki, boshqa ko'plab modellar nazoratli o'qitishni (`supervision`) talab qiladi. Nazoratli o'qitish — bu _ML_ algoritmlarini maxsus belgilangan ma'lumotlar yordamida o'qitish jarayonidir va bunday ma'lumotlarni to'plash qimmatga tushishi va ko'p vaqt talab qilishi mumkin. O'z-o'zini nazorat qilish esa aynan shu ma'lumotlarni belgilashdagi to'siqni yengib o'tishga yordam beradi. Bu esa, o'z navbatida, modellarga o'rganish uchun kattaroq ma'lumotlar to'plamlarini yaratish va ularning miqyosini samarali ravishda kengaytirish imkonini beradi. Keling, buni batafsil ko'rib chiqamiz.

Nazoratli o'qitishda siz model o'rganishi kerak bo'lgan xususiyatlarni ko'rsatish uchun namunalarni belgilab chiqasiz va keyin modelni shu namunalar asosida o'qitasiz. O'qitib bo'lingach, modelni yangi ma'lumotlarga qo'llash mumkin bo'ladi. Masalan, firibgarlikni aniqlash modelini o'qitish uchun siz har biri “firibgarlik” yoki “firibgarlik emas” deb belgilangan tranzaksiyalar namunalaridan foydalanasiz. Model bu namunalardan o'rganib olgach, siz undan yangi tranzaksiyaning firibgarlik ekanligini bashorat qilish uchun foydalanishingiz mumkin.

2010-yillarda SI modellarining muvaffaqiyati aynan nazoratli o'qitishga asoslangan edi. Chuqur o'rganish (`deep learning`) inqilobini boshlab bergan model — _AlexNet_ ([Krizhevsky va boshq., 2012](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)) — nazoratli o'qitishga asoslangan edi. U _ImageNet_ ma'lumotlar to'plamidagi 1 milliondan ortiq rasmni tasniflashni o'rganish uchun o'qitilgan. U har bir rasmni “mashina”, “havo shari” yoki “maymun” kabi 1000 ta toifadan biriga ajratgan.

Nazoratli o'qitishning kamchiligi shundaki, ma'lumotlarni belgilash qimmat va ko'p vaqt talab qiladigan jarayondir. Agar bir kishi bitta rasmni belgilashi uchun 5 sent ketsa, _ImageNet_ uchun millionta rasmni belgilash 50 000 dollarga tushadi.[^4] Agar siz har bir rasmni ikki xil odam belgilashini istasangiz — bu orqali belgilash sifatini qayta tekshirish mumkin bo'ladi — xarajat ikki baravar oshadi. Dunyoda 1000 tadan ancha ko'p obyekt mavjudligini hisobga olsak, modellarning imkoniyatlarini ko'proq obyektlar bilan ishlashga kengaytirish uchun siz ko'proq toifalarga oid belgilarni qo'shishingiz kerak bo'ladi. Miqyosni 1 million toifaga yetkazish uchun esa, faqat belgilash xarajatining o'zi 50 million dollarga yetadi.

Kundalik obyektlarni belgilashni ko'pchilik odamlar maxsus tayyorgarliksiz ham bajara oladi. Shu sababli, buni nisbatan arzon amalga oshirish mumkin. Biroq, hamma belgilash vazifalari ham bunchalik oson emas. Ingliz tilidan lotin tiliga tarjima qiluvchi model uchun lotincha tarjimalarni yaratish ancha qimmatroq. Kompyuter tomografiyasi (`CT scan`) suratida saraton belgilari bor-yo'qligini belgilash esa tasavvur qilib bo'lmas darajada qimmatga tushardi.

O'z-o'zini nazorat qilish ma'lumotlarni belgilashdagi bu to'siqni yengib o'tishga yordam beradi. O'z-o'zini nazorat qilishda model aniq belgilarni talab qilish o'rniga, belgilarni kiruvchi ma'lumotlarning o'zidan chiqarib oladi. Til modellashtirish o'z-o'zini nazorat qilishga asoslanadi, chunki har bir kiruvchi ketma-ketlik ham belgilarni (bashorat qilinishi kerak bo'lgan tokenlarni), ham model bu belgilarni bashorat qilish uchun foydalanishi mumkin bo'lgan kontekstni o'zida mujassam etadi. Masalan, "Men ko'cha taomlarini yaxshi ko'raman." ("I love street food.") jumlasi 1-1-jadvalda ko'rsatilganidek, oltita o'qitish namunasini beradi.

| Kirish (kontekst) | Chiqish (keyingi token) |
| :--- | :--- |
| `<BOS>` | I |
| `<BOS>`, I | love |
| `<BOS>`, I, love | street |
| `<BOS>`, I, love, street | food |
| `<BOS>`, I, love, street, food | . |
| `<BOS>`, I, love, street, food, . | `<EOS>` |

<div className='text-center text-sm italic'>1-1 jadval. "I love street food." jumlasidan til modellashtirish uchun olingan o'qitish namunalari.</div>

1-1-jadvalda `<BOS>` va `<EOS>` ketma-ketlikning boshlanishi va tugashini bildiradi. Bu belgilar til modelining bir nechta ketma-ketliklar bilan ishlashi uchun zarur. Har bir belgi odatda model tomonidan bitta maxsus token sifatida qabul qilinadi. Ketma-ketlikning tugash belgisi ayniqsa muhim, chunki u til modellariga o'z javoblarini qachon tugatish kerakligini bilishga yordam beradi.[^5]

<Callout>
### Eslatma

O'z-o'zini nazorat qilish (`self-supervision`) nazoratsiz o'qitishdan (`unsupervision`) farq qiladi. O'z-o'zini nazorat qilishda belgilar kiruvchi ma'lumotlarning o'zidan chiqarib olinsa, nazoratsiz o'qitishda esa belgilarga umuman hojat yo'q.
</Callout>

O'z-o'zini nazorat qilish til modellarining hech qanday belgilashni talab qilmasdan, to'g'ridan-to'g'ri matn ketma-ketliklaridan o'rganishini anglatadi. Matn ketma-ketliklari hamma joyda — kitoblarda, blog postlarida, maqolalarda va `Reddit` izohlarida — mavjud bo'lgani uchun, ulkan hajmdagi o'qitish ma'lumotlarini to'plash mumkin. Bu esa til modellarining miqyosini kengaytirib, katta til modellariga (_LLM_) aylanishiga imkon beradi.

Biroq, _LLM_ atamasi ilmiy jihatdan unchalik aniq emas. Til modeli “katta” deb hisoblanishi uchun qanchalik katta bo'lishi kerak? Bugun katta deb hisoblangan narsa ertaga kichik bo'lib qolishi mumkin. Modelning hajmi odatda uning parametrlari soni bilan o'lchanadi. Parametr — bu _ML_ modelidagi o'zgaruvchi bo'lib, u o'qitish jarayonida yangilanib boradi.[^6] Umuman olganda, garchi bu har doim ham to'g'ri bo'lmasa-da, modelda qancha ko'p parametr bo'lsa, uning kerakli xususiyatlarni o'rganish salohiyati shuncha yuqori bo'ladi.

2018-yil iyun oyida `OpenAI`'ning birinchi generativ oldindan o'qitilgan transformeri (`GPT`) chiqqanida, uning 117 million parametri bor edi va bu katta hisoblanardi. 2019-yil fevral oyida `OpenAI` 1.5 milliard parametrli `GPT-2`'ni taqdim etganida, 117 million endi kichik deb hisoblana boshlandi. Ushbu kitob yozilayotgan vaqtda, 100 milliard parametrli model katta hisoblanadi. Balki bir kun kelib bu hajm ham kichik bo'lib qolar.

Keyingi bo'limga o'tishdan oldin, odatda e'tibordan chetda qoladigan bir savolga to'xtalib o'tmoqchiman: _Nima uchun kattaroq modellarga ko'proq ma'lumot kerak?_ Kattaroq modellar o'rganish uchun ko'proq salohiyatga ega, shuning uchun ularning samaradorligini maksimal darajaga chiqarish uchun ko'proq o'qitish ma'lumotlari kerak bo'ladi.[^7] Katta modelni kichik ma'lumotlar to'plamida ham o'qitish mumkin, lekin bu hisoblash resurslarini behuda sarflash bo'ladi. Bu ma'lumotlar to'plamida kichikroq modellar yordamida ham xuddi shunday yoki undan ham yaxshiroq natijalarga erishish mumkin edi.

## Katta til modellari (_LLM_) dan fundamental modellarga

Til modellari misli ko'rilmagan vazifalarni bajara olsa-da, ular matn bilan cheklangan. Biz, insonlar, dunyoni nafaqat til orqali, balki ko'rish, eshitish, sezish va boshqa tuyg'ular yordamida idrok etamiz. Matndan tashqari ma'lumotlarni qayta ishlay olish SI'ning real dunyoda faoliyat yuritishi uchun juda muhimdir.

Shu sababli, til modellari ko'proq ma'lumot modalliklarini o'z ichiga olish uchun kengaytirilmoqda. `GPT-4V` va `Claude 3` kabi modellar ham tasvirlarni, ham matnlarni tushuna oladi. Ba'zi modellar hatto videolar, 3D obyektlar, oqsil tuzilmalari va hokazolarni ham tushunadi. Til modellariga ko'proq ma'lumot modalliklarini qo'shish ularni yanada qudratliroq qiladi. "OpenAI" 2023-yilda [o'zining `GPT-4V` tizimi haqidagi hisobotida](https://cdn.openai.com/papers/GPTV_System_Card.pdf) shunday degan edi: “_LLM_'larga qo'shimcha modalliklarni (masalan, tasvirli kiritishlarni) qo'shish ba'zilar tomonidan SI tadqiqotlari va ishlanmalaridagi asosiy, ilg'or yo'nalishlardan biri sifatida qaralmoqda.”

Garchi ko'pchilik `Gemini` va `GPT-4V`'ni hali ham _LLM_ deb atasa-da, ularni [fundamental modellar](https://arxiv.org/abs/2108.07258) deb ta'riflash to'g'riroq bo'ladi. “Fundamental” so'zi ham bu modellarning SI dasturlaridagi muhimligini, ham ularning asosida turli ehtiyojlar uchun yangi narsalar qurish mumkinligini anglatadi.

Fundamental modellar SI tadqiqotlarining an'anaviy tuzilishida yangi bir bosqichni boshlab berdi. Uzoq vaqt davomida SI tadqiqotlari ma'lumot modalliklari bo'yicha bo'lingan edi. Tabiiy tilni qayta ishlash (`NLP`) faqat matn bilan shug'ullanardi. Kompyuter ko'rishi (`Computer vision`) faqat tasvir bilan shug'ullanardi. Faqat matn bilan ishlaydigan modellar tarjima va spamni aniqlash kabi vazifalar uchun ishlatilishi mumkin edi. Faqat tasvir bilan ishlaydigan modellar esa obyektlarni aniqlash va tasvirlarni tasniflash uchun ishlatilardi. Faqat audio bilan ishlaydigan modellar nutqni aniqlash (nutqdan-matnga yoki `STT`) va nutq sintezi (matndan-nutqqa yoki `TTS`) kabi vazifalarni bajara olardi.

Birdan ortiq ma'lumot modalligi bilan ishlay oladigan model **multimodal model** deb ham ataladi. Generativ multimodal model esa **katta multimodal model** (`LMM`) deyiladi. Agar til modeli keyingi tokenni faqat matnli tokenlarga asoslanib generatsiya qilsa, multimodal model keyingi tokenni ham matn, ham tasvir tokenlariga, ya'ni model qo'llab-quvvatlaydigan barcha modalliklarga asoslanib generatsiya qiladi (1-3-rasm).

![](/ai-engineering/1.3-figure.png)

<div className='text-center text-sm italic'>1-3-rasm. Multimodal model keyingi tokenni ham matn, ham vizual tokenlardan olingan ma'lumotlar asosida generatsiya qila oladi</div>

Til modellari singari, multimodal modellar ham miqyoslash uchun ma'lumotlarga muhtoj. O'z-o'zini nazorat qilish yondashuvi multimodal modellar uchun ham ishlaydi. Masalan, OpenAI o'zining til-tasvir modeli bo'lgan `CLIP`'ni o'qitish uchun o'z-o'zini nazorat qilishning tabiiy til nazorati (`natural language supervision`) deb nomlangan bir turidan foydalangan ([OpenAI, 2021](https://openai.com/index/clip/)). Ular har bir tasvir uchun yorliqlarni mustaqil yaratish o'rniga, internetda birga uchraydigan (tasvir, matn) juftliklarini topdilar. Ular mustaqil belgilash xarajatlarisiz 400 million (tasvir, matn) juftligidan iborat ma'lumotlar to'plamini yarata oldilar, bu esa _ImageNet_'dan 400 baravar kattaroq edi. Bu ma'lumotlar to'plami `CLIP`'ning qo'shimcha o'qitishni talab qilmasdan, bir nechta tasvir tasnifi vazifalariga umumlashtira oladigan birinchi modelga aylanishiga imkon berdi.

<Callout>
### Eslatma

Ushbu kitobda fundamental modellar atamasi ham katta til modellari, ham katta multimodal modellarni anglatadi.
</Callout>

Shuni ta'kidlash joizki, `CLIP` generativ model emas — u erkin natijalar generatsiya qilishga o'rgatilmagan. `CLIP` — bu ham matnlar, ham tasvirlarning qo'shma embedding'larini hosil qilishga o'rgatilgan embedding modelidir (embedding — bu so'z yoki tasvir kabi murakkab ma'lumotni ma'nosini saqlagan holda ixcham raqamli ko'rinishga, ya'ni vektorga o'tkazish jarayoni). “Embedding'larga kirish” bo'limida embedding'lar batafsil muhokama qilinadi. Hozircha, embedding'larni asl ma'lumotlarning ma'nosini o'zida aks ettirishga harakat qiladigan vektorlar deb tasavvur qilishingiz mumkin. `CLIP` kabi multimodal embedding modellari `Flamingo`, `LLaVA` va `Gemini` (avvalgi `Bard`) singari generativ multimodal modellarning asosini, ya'ni tayanchini tashkil etadi.

### Maxsus vazifali modellardan umumiy maqsadli modellarga

Fundamental modellar, shuningdek, maxsus vazifali modellardan umumiy maqsadli modellarga o'tish davrini ham anglatadi. Ilgari modellar ko'pincha hissiyot tahlili (`sentiment analysis`) yoki tarjima kabi maxsus vazifalar uchun ishlab chiqilardi. Hissiyot tahlili uchun o'qitilgan model tarjima qila olmasdi va aksincha.

Fundamental modellar o'zlarining miqyosi va o'qitilish usuli tufayli juda keng doiradagi vazifalarni bajara oladi. Tayyor holatida, umumiy maqsadli modellar ko'plab vazifalarni nisbatan yaxshi bajara oladi. _LLM_ ham hissiyot tahlilini, ham tarjimani bajara oladi. Biroq, ko'p hollarda umumiy maqsadli modelni biror maxsus vazifa uchun uning samaradorligini maksimal darajaga chiqarish maqsadida biroz o'zgartirish (`tweak`) mumkin.

1-4-rasmda fundamental modellarni baholash uchun _Super-NaturalInstructions_ mezonida (`benchmark`) ishlatiladigan vazifalar ko'rsatilgan va bu fundamental model qanday turdagi vazifalarni bajara olishi haqida tasavvur beradi ([Wang va boshq., 2022](https://arxiv.org/abs/2204.07705)).

Tasavvur qiling, siz bir chakana savdo kompaniyasi bilan ularning veb-sayti uchun mahsulot tavsiflarini generatsiya qiladigan dastur ustida ishlayapsiz. Tayyor holatidagi model to'g'ri tavsiflarni generatsiya qilishi mumkin, lekin brendning o'ziga xos ovozini (`voice`) aks ettira olmasligi yoki brendning asosiy g'oyalarini (`messaging`) ta'kidlay olmasligi mumkin. Generatsiya qilingan tavsiflar hatto marketing nutqi va siyqasi chiqqan iboralarga (`cliches`) to'la bo'lishi ham mumkin.

![Super-NaturalInstructions mezonidagi vazifalar doirasi (Wang va boshq., 2022)](/ai-engineering/1.4-figure.png)

<div className='text-center text-sm italic'>1-4-rasm. _Super-NaturalInstructions_ mezonidagi vazifalar doirasi (Wang va boshq., 2022).</div>

### Modelni vazifaga moslashtirish

Modeldan o'zingiz istagan natijani olish uchun qo'llashingiz mumkin bo'lgan bir nechta texnikalar mavjud. Masalan, siz kerakli mahsulot tavsiflariga oid misollar bilan boyitilgan batafsil ko'rsatmalar ishlab chiqishingiz mumkin. Bu yondashuv prompt muhandisligi (`prompt engineering`) deb ataladi. Yoki modelni mijozlarning fikr-mulohazalari bazasiga ulashingiz mumkin, model esa bu ma'lumotlardan dastak sifatida foydalanib, yanada yaxshiroq tavsiflar generatsiya qiladi. Ko'rsatmalarni ma'lumotlar bazasi bilan to'ldirish qidiruv bilan to'ldirilgan generatsiya (_Retrieval-Augmented Generation_, `RAG`) deb nomlanadi. Shuningdek, siz modelni yuqori sifatli mahsulot tavsiflari to'plamida qo'shimcha sozlash (`finetuning`), ya'ni qo'shimcha o'qitish imkoniga ham egasiz.

### Tayyor modeldan foydalanish yoki o'zi yaratish?

Prompt muhandisligi, `RAG` va qo'shimcha sozlash — bular modelni o'z ehtiyojlaringizga moslashtirish uchun ishlatishingiz mumkin bo'lgan, sun'iy intellekt muhandisligidagi uchta eng keng tarqalgan texnikadir. Kitobning qolgan qismida ularning barchasi batafsil muhokama qilinadi.

Mavjud qudratli modelni o'z vazifangizga moslashtirish, odatda, vazifangiz uchun modelni noldan yaratishdan ancha osonroqdir — masalan, 1 million namuna va olti oyga qarshi o'nta namuna va bir dam olish kuni. Fundamental modellar SI dasturlarini ishlab chiqishni arzonlashtiradi va mahsulotni bozorga chiqarish vaqtini (`time to market`) qisqartiradi. Modelni moslashtirish uchun aniq qancha ma'lumot kerakligi siz qo'llaydigan texnikaga bog'liq. Ushbu kitob har bir texnikani muhokama qilganda bu savolga ham to'xtalib o'tadi. Shunga qaramay, maxsus vazifali modellarning ham ko'plab afzalliklari bor, masalan, ular ancha kichikroq bo'lishi mumkin, bu esa ulardan foydalanishni tezroq va arzonroq qiladi.

O'z modelingizni yaratish yoki mavjudidan foydalanish — bu jamoalar o'zlari javob topishlari kerak bo'lgan klassik "sotib olish yoki o'zi yaratish" masalasidir. Kitob davomidagi muhokamalar bu qarorni qabul qilishga yordam beradi.




[^1]: Ingliz tilidan boshqa tillarda bitta `Unicode` belgisi ba'zan bir nechta token sifatida ifodalanishi mumkin.

[^2]: Avtoregressiv til modellari ba'zan sababiy til modellari ([`causal language models`](https://huggingface.co/docs/transformers/en/tasks/language_modeling)) deb ham ataladi.

[^3]: Texnik jihatdan, agar juda qattiq harakat qilinsa, `BERT` kabi niqoblangan til modelidan ham matn generatsiyasi uchun foydalanish mumkin.

[^4]: Ma'lumotlarni belgilashning haqiqiy narxi bir nechta omillarga, jumladan, vazifaning murakkabligi, miqyosi (kattaroq ma'lumotlar to'plamlari odatda har bir namuna uchun arzonroq narxga olib keladi) va belgilash xizmatini taqdim etuvchi provayderga bog'liq. Masalan, 2024-yil sentabr holatiga ko'ra, [`Amazon SageMaker Ground Truth`](https://aws.amazon.com/sagemaker/ai/groundtruth/) 50 000 tadan kam rasmni belgilash uchun har bir rasmga 8 sent, 1 milliondan ortiq rasmni belgilash uchun esa har bir rasmga atigi 2 sent oladi.

[^5]: Bu xuddi insonlar uchun qachon gapirishni to'xtatishni bilish muhim bo'lganiga o'xshaydi.

[^6]: Maktabda menga model parametrlari ham model og'irliklari (`weights`), ham model siljishlari (`biases`)ni o'z ichiga oladi deb o'rgatishgan. Biroq, bugungi kunda biz odatda barcha parametrlarni nazarda tutib, model og'irliklari atamasini ishlatamiz.

[^7]: Kattaroq modellarga ko'proq o'qitish ma'lumotlari kerakligi mantiqqa zid tuyulishi mumkin. Agar model qudratliroq bo'lsa, o'rganish uchun kamroq namuna talab qilishi kerak emasmi? Biroq, bizning maqsadimiz katta modelning bir xil ma'lumotlar yordamida kichik modelning samaradorligiga erishishi emas. Bizning maqsadimiz — model samaradorligini maksimal darajaga chiqarishdir.